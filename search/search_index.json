{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to the Real Time Data Ingestion Platform <p>Easy access to high volume, historical and real time process data for analytics applications, engineers, and data scientists wherever they are.</p>                  Getting Started"},{"location":"api/authentication/","title":"Authentication","text":"<p>RTDIP REST APIs require Azure Active Directory Authentication and passing the token received as an <code>authorization</code> header in the form of a Bearer token. An example of the REST API header is <code>Authorization: Bearer &lt;&lt;token&gt;&gt;</code></p>"},{"location":"api/authentication/#end-user-authentication","title":"End User Authentication","text":"<p>If a developer or business user would like to leverage the RTDIP REST API suite, it is recommended that they use the Identity Packages provided by Azure to obtain a token.</p> <ul> <li>REST API</li> <li>.NET</li> <li>Java</li> <li>Python</li> <li>Javascript</li> </ul> <p>Note</p> <p>Note that the above packages have the ability to obtain tokens for end users and service principals and support all available authentication options. </p> <p>Ensure to install the relevant package and obtain a token.</p> <p>See the examples section to see various authentication methods implemented.</p>"},{"location":"api/examples/","title":"Examples","text":"<p>Below are examples of how to execute APIs using various authentication options and API methods.</p>"},{"location":"api/examples/#end-user-authentication","title":"End User Authentication","text":""},{"location":"api/examples/#python","title":"Python","text":"<p>A python example of obtaining a token as a user can be found below using the <code>azure-identity</code> python package to authenticate with Azure AD.</p> <p>POST Requests</p> <p>The POST request can be used to pass many tags to the API. This is the preferred method when passing large volumes of tags to the API.</p> GET RequestPOST Request <pre><code>from azure.identity import DefaultAzureCredential\nimport requests\n\nauthentication = DefaultAzureCredential()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\nparams = {\n\"business_unit\": \"Business Unit\",\n\"region\": \"Region\",\n\"asset\": \"Asset Name\",\n\"data_security_level\": \"Security Level\",\n\"data_type\": \"float\",\n\"tag_name\": \"TAG1\",\n\"tag_name\": \"TAG2\",\n\"start_date\": \"2022-01-01\",\n\"end_date\": \"2022-01-01\",\n\"include_bad_data\": True\n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={}\nheaders = {\n'Authorization': 'Bearer {}'.format(access_token)\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, params=params, data=payload)\n\nprint(response.json())\n</code></pre> <pre><code>from azure.identity import DefaultAzureCredential\nimport requests\n\nauthentication = DefaultAzureCredential()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\nparams = {\n\"business_unit\": \"Business Unit\",\n\"region\": \"Region\",\n\"asset\": \"Asset Name\",\n\"data_security_level\": \"Security Level\",\n\"data_type\": \"float\",\n\"start_date\": \"2022-01-01T15:00:00\",\n\"end_date\": \"2022-01-01T16:00:00\",\n\"include_bad_data\": True    \n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={\"tag_name\": [\"TAG1\", \"TAG2\"]}\n\nheaders = {\n\"Authorization\": \"Bearer {}\".format(access_token),\n}\n\n# Requests automatically sets the Content-Type to application/json when the request body is passed via the json parameter\nresponse = requests.request(\"POST\", url, headers=headers, params=params, json=payload)\n\nprint(response.json())\n</code></pre>"},{"location":"api/examples/#service-principal-authentication","title":"Service Principal Authentication","text":""},{"location":"api/examples/#get-request","title":"GET Request","text":"<p>Authentication using Service Principals is similar to end user authentication. An example, using Python is provided below where the <code>azure-identity</code> package is not used, instead a direct REST API call is made to retrieve the token.</p> cURLPython <pre><code>curl --location --request POST 'https://login.microsoftonline.com/{tenant id}/oauth2/v2.0/token' \\\n--form 'grant_type=\"client_credentials\"' \\\n--form 'client_id=\"&lt;&lt;client id&gt;&gt;\"' \\\n--form 'client_secret=\"&lt;&lt;client secret&gt;&gt;\"' \\\n--form 'scope=\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\"'\n</code></pre> <pre><code>import requests\n\nurl = \"https://login.microsoftonline.com/{tenant id}/oauth2/v2.0/token\"\n\npayload={'grant_type': 'client_credentials',\n'client_id': '&lt;&lt;client id&gt;&gt;',\n'client_secret': '&lt;&lt;client secret&gt;&gt;',\n'scope': '2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default'}\nfiles=[]\nheaders = {}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n\naccess_token  = response.json()[\"access_token\"])\n\nparams = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\",\n    \"asset\": \"Asset Name\",\n    \"data_security_level\": \"Security Level\",\n    \"data_type\": \"float\",\n    \"tag_name\": \"TAG1\",\n    \"tag_name\": \"TAG2\",\n    \"start_date\": \"2022-01-01\",\n    \"end_date\": \"2022-01-01\",\n    \"include_bad_data\": True\n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={}\nheaders = {\n'Authorization': 'Bearer {}'.format(access_token)\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, params=params, data=payload)\n\nprint(response.text)\n</code></pre>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#rtdip-rest-apis","title":"RTDIP REST APIs","text":"<p>RTDIP provides REST API endpoints for querying data in the platform. The APIs are a wrapper to the python RTDIP SDK and provide similar functionality for users and applications that are unable to leverage the python RTDIP SDK. It is recommended to read the RTDIP SDK documentation and in particular the Functions section for more information about the options and logic behind each API.</p>"},{"location":"api/rest_apis/","title":"RTDIP REST API Endpoints","text":"<p>RTDIP REST API documentation is available in a number of formats, as described below. </p> <p> </p> <p>RTDIP REST APIs are built to OpenAPI standard 3.0.2. You can obtain the OpenAPI JSON schema at the following endpoint of your deployed APIs <code>https://{domain name}/api/openapi.json</code></p> <p> </p> <p>It is recommended to review the Swagger documentation that can be found at the following endpoint of your deployed APIs <code>https://{domain name}/docs</code> for more information about the parameters and options for each API. It is also possible to try out each API from this link.</p> <p> </p> <p>Additionally, further information about each API can be found in Redoc format at  the following endpoint of your deployed APIs <code>https://{domain name}/redoc</code></p>"},{"location":"api/deployment/azure/","title":"Deploy RTDIP APIs to Azure","text":"<p>The RTDIP repository contains the code to deploy the RTDIP REST APIs to your own Azure Cloud environment. The APIs are built as part of the rtdip repository CI/CD pipelines and the image is deployed to Docker Hub repo <code>rtdip/api</code>. Below contains information on how to build and deploy the containers from source or to setup your function app to use the deployed container image provided by RTDIP.</p>"},{"location":"api/deployment/azure/#deploying-the-rtdip-apis","title":"Deploying the RTDIP APIs","text":""},{"location":"api/deployment/azure/#deployment-from-build","title":"Deployment from Build","text":"<p>To deploy the RTDIP APIs directly from the repository, follow the steps below:</p> <ol> <li>Build the docker image using the following command:     <pre><code>docker build --tag &lt;container_registry_url&gt;/rtdip-api:v0.1.0 -f src/api/Dockerfile .\n</code></pre></li> <li>Login to your container registry     <pre><code>docker login &lt;container_registry_url&gt;\n</code></pre></li> <li>Push the docker image to your container registry     <pre><code>docker push &lt;container_registry_url&gt;/rtdip-api:v0.1.0\n</code></pre></li> <li>Configure your Function App to use the docker image     <pre><code>az functionapp config container set --name &lt;function_app_name&gt; --resource-group &lt;resource_group_name&gt; --docker-custom-image-name &lt;container_registry_url&gt;/rtdip-api:v0.1.0\n</code></pre></li> </ol>"},{"location":"api/deployment/azure/#deployment-from-docker-hub","title":"Deployment from Docker Hub","text":"<p>To deploy the RTDIP APIs from Docker Hub, follow the steps below:</p> <ol> <li>Configure your Function App to use the docker image     <pre><code>az functionapp config container set --name &lt;function_app_name&gt; --resource-group &lt;resource_group_name&gt; --docker-custom-image-name rtdip/api:azure-&lt;version&gt;\n</code></pre></li> </ol>"},{"location":"blog/delta_and_rtdip/","title":"Delta Lakehouse and Real Time Data Ingestion Platform","text":"<p>Real Time Data Ingestion Platform leverages Delta and the concept of a Lakehouse to ingest, store and manage it's data. There are many benefits to Delta for performing data engineering tasks on files stored in a data lake including ACID transactions, maintenance, SQL query capability and performance at scale. To find out more about Delta Lakehouse please see here.</p> <p>The Real Time Data Ingestion Platform team would like to share some lessons learnt from the implementation of Delta and the Lakehouse concept so that hopefully it helps others on their Delta Lakehouse journey.</p> <p>For reference, please consider the typical layout of timeseries data ingested by RTDIP:</p> <p>Events</p> Column Name Desciption TagName Typically represents a sensor name or a measurement EventTime A timestamp for a recorded value Status Status of the recording, normally indicating if the measurement value is good or bad Value The value of the measurement and can be of a number of types - float, double, string, integer <p>Metadata</p> Column Name Desciption TagName Typically represents a sensor name or a measurement Description A description for the sensor UoM UoM for the measurement <p>Note</p> <p>Metadata can include a number of additional columns and depends on the system that provides the metadata. The above are the required columns for any sensor data ingested by RTDIP.</p>"},{"location":"blog/delta_and_rtdip/#design-considerations","title":"Design Considerations","text":"<p>Delta, in its simplest definition, is a set of parquet files managed by an index file. This allows Spark to perform tasks like partition pruning and file pruning to find the exact parquet file to be used by any ACID transactions being performed on it. By reducing the number of files and the amount of data that Spark needs to read in a query means that it will perform much better. It is important to consider the following when designing a Delta Table to achieve performance benefits:</p> <ul> <li>Columns that are likely to be used in most reads and writes</li> <li>Partition column(s)</li> <li>File Sizes</li> </ul>"},{"location":"blog/delta_and_rtdip/#partition-columns","title":"Partition Columns","text":"<p>The biggest benefit achieved using Delta is to include a partition column in the design of a Delta Table. This is the fastest way for Spark to isolate the dataset it needs to work with in a query. The general rule of thumb is that each partition size should be roughly 1gb in size, and ideally would be a column or columns that are used in every query to filter data for that table.</p> <p>This can be difficult to achieve. The most queried columns in RTDIP event data are TagName and EventTime, however, partitioning data by TagName creates far too many small parititons and a timestamp column like EventTime can not be used for partitioning for the same reason. The best outcome is typically to create an additional column that is an aggregation of the EventTime column, such as a Date, Month or Year Column, depending on the frequency of the data being ingested. </p> <p>Note</p> <p>Given the above, always query RTDIP delta events tables using EventDate in the filter to achieve the best results.</p> <p>One of the best methods to analyse Spark query performance is to analyse the query plan of a query. It is essential that a Spark query plan leverages a partition column. This can be identified by reviewing the query plan in Spark. As per the below query plan, it can be seen that for this particular query only one partition was read by Spark. Make sure to try different queries to identify that the expected number of partitions are being used by Spark in every query. If it does not match your expected number of partitions, it is important to investigate why partition pruning is not being leveraged in your query. </p> <p> </p>"},{"location":"blog/delta_and_rtdip/#zorder-columns","title":"ZORDER Columns","text":"<p>Even though the rule is to achieve roughly 1gb partitions for a Delta Table, Delta is likely to divide that partition into a number of files. The default target size is around 128gb per file. Due to this, it is possible to improve performance above and beyond partitioning by telling Spark which files within in a partition to read. This is where ZORDER becomes useful. </p> <p>Zordering organises the data within each file, and along with the Delta Index file, directs Spark to the exact files to use in its reads(and merge writes) on the table. It is important to find the right number of columns to ZORDER - the best outcome would be a combination of columns that does not cause the index to grow too large. For example, ZORDERING by TagName creates a small index, but ZORDERING by TagName and EventTime created a huge index as there are far more combinations to be indexed.</p> <p>The most obvious column to ZORDER on in RTDIP is the TagName as every query is likely to use this in its filter. Like parition pruning, it is possible to identify the impact of ZORDERING on your queries by reviewing the files read attribute in the query plan. As per the query plan below, you can see that two files were read within the one partition.</p> <p> </p>"},{"location":"blog/delta_and_rtdip/#merge-and-file-sizes","title":"MERGE and File Sizes","text":"<p>As stated above, the default target size for file sizes within a partition is 128gb. However, this is not always ideal and in certain scenarios, it is possible to improve performance of Spark jobs by reducing file sizes in cetain scenarios: - MERGE commands - Queries that target very small subsets of data within a file</p> <p>Due to the nature of Merges, its typically an action where small updates are being made to the dataset. Due to this, it is possible to get much better MERGE performance by setting the following attribute on the Delta Table <code>delta.tuneFileSizesForRewrites=true</code>. This targets smaller file sizes to reduce the amount of data in each read a MERGE operation performs on the data. RTDIP gained a significant performance improvement in reading and writing and was able to reduce the Spark cluster size by half by implementing this setting on its Delta Tables.</p> <p>However, even more performance gain was achieved when Databricks released Low Shuffle Merge from DBR 9.0 onwards. This assists Spark to merge data into files without disrupting the ZORDER layout of Delta files, in turn assisting Merge commands to continue leveraging ZORDER performance benefits on an ongoing basis. RTDIP was able to improve MERGE performance by 5x with this change. To leverage Low Shuffle Merge, set the following Spark config in your notebook <code>spark.databricks.delta.merge.enableLowShuffle=true</code>.</p>"},{"location":"blog/delta_and_rtdip/#delta-table-additional-attributes","title":"Delta Table Additional Attributes","text":"<p>It is recommendded to consider setting the following two attributes on all Delta Tables:</p> <ul> <li><code>delta.autoOptimize.autoCompact=true</code></li> <li><code>delta.autoOptimize.optimizeWrite=true</code></li> </ul> <p>To understand more about optimization options you can set on Delta Tables, please refer to this link.</p>"},{"location":"blog/delta_and_rtdip/#maintenance-tasks","title":"Maintenance Tasks","text":"<p>One important step to be included with every Delta Table is maintenance. Most developers forget these very important maintenance tasks that need to run on a regular basis to maintain performance and cost on Delta Tables.</p> <p>As a standard, run a maintenance job every 24 hours to perform OPTIMIZE and VACUUM commands on Delta Tables.</p>"},{"location":"blog/delta_and_rtdip/#optimize","title":"OPTIMIZE","text":"<p>OPTIMIZE is a Spark SQL command that can be run on any Delta Table and is the simplest way to optimize the file layout of a Delta Table. The biggest benefit of running OPTIMIZE however, is to organize Delta files using ZORDER. Due to how effecive ZORDER is on queries, its unlikely that OPTIMIZE would not be executed on a Delta Table regularly.</p> <p>It may be a question as to why one would run OPTIMIZE as well as set <code>delta.autoOptimize.autoCompact=true</code> on all its tables. Auto Compact does not ZORDER its data(at the time of writing this article), its task is simply to attempt to create larger files during writing and avoid the small file problem. Therefore, autoCompact does not provide ZORDER capability. Due to this, consider an OPTIMIZE strategy as follows: - Auto Compact is used by default for any new files written to an RTDIP Delta Table between the execution of maintenance jobs. This ensures that any new data ingested by RTDIP is still being written in a suitable and performant manner. - OPTIMIZE with ZORDER is run on a daily basis on any partitions that have changed(excluding the current day) to ensure ZORDER and updating of the Delta Index file is done. </p> <p>Note</p> <p>RTDIP data is going to typically be ingesting using Spark Streaming - given the nature of a real time data ingestion platform, it makes sense that data ingestion is performed in real time. One complication this introduces is the impact of the OPTIMIZE command being executed at the same time as files being written to a partition. Due to this, execute OPTIMIZE on partitions where the EventDate is not equal to the current date, minimizing the possibility of an OPTIMIZE command and a file write command being executed on a partition at the same time. This logic reduces issues experienced by both the maintenance job and Spark streaming job.</p>"},{"location":"blog/delta_and_rtdip/#vacuum","title":"VACUUM","text":"<p>One of the most powerful features of Delta is time travel. This allows querying of data as at a certain point of time in the past, or a particular version of the Delta Table. Whilst incredibly useful, it does consume storage space and if these historical files are never removed, the size of Delta Tables can grow exponentially large and increase cost.</p> <p>To ensure only the required historical versions of a Delta Table are stored, its important to execute the VACUUM command every 24 hours. This deletes any files or versions that are outside the time travel retention period.</p>"},{"location":"blog/delta_and_rtdip/#conclusion","title":"Conclusion","text":"<p>Delta and the Lakehouse transformed the way RTDIP ingests its data and provides integration with other projects, applications and platforms. We hope the above assists others with their Delta development and we look forward to posting more content on RTDIP and its use of Spark in the future.</p>"},{"location":"blog/overview/","title":"Overview","text":"<p>"},{"location":"blog/overview/#blogs-about-the-real-time-data-ingestion-platform","title":"Blogs about the Real Time Data Ingestion Platform","text":""},{"location":"blog/rtdip_ingestion_pipelines/","title":"RTDIP Ingestion Pipeline Framework","text":"<p>RTDIP has been built to simplify ingesting and querying time series data. One of the most anticipated features of the Real Time Data Ingestion Platform for 2023 is the ability to create streaming and batch ingestion pipelines according to requirements of the source of the data and needs of the data consumer. Of equal importance is the need to query this data and an article that focuses on egress will follow in due course. </p>"},{"location":"blog/rtdip_ingestion_pipelines/#overview","title":"Overview","text":"<p>The goal of the RTDIP Ingestion Pipeline framework is:</p> <ol> <li>Support python and pyspark to build pipeline components</li> <li>Enable execution of sources, transformers, sinks/destinations and utilities components in a framework that can execute them in a defined order</li> <li>Create modular components that can be leveraged as a step in a pipeline task using Object Oriented Programming techniques included Interfaces and Implementations per component type</li> <li>Deploy pipelines to popular orchestration engines</li> <li>Ensure pipelines can be constructed and executed using the RTDIP SDK and rest APIs</li> </ol>"},{"location":"blog/rtdip_ingestion_pipelines/#pipeline-jobs","title":"Pipeline Jobs","text":"<p>The RTDIP Data Ingestion Pipeline Framework will follow the typical convention of a job that users will be familiar with if they have used orchestration engines such as Apache Airflow or Databricks Workflows.</p> <p>A pipline job consists of the following components:</p> <pre><code>erDiagram\n  JOB ||--|{ TASK : contains\n  TASK ||--|{ STEP : contains\n  JOB {\n    string name\n    string description\n    list task_list\n  }\n  TASK {\n    string name\n    string description\n    string depends_on_task\n    list step_list\n    bool batch_task\n  }\n  STEP {\n    string name\n    string description\n    list depends_on_step\n    list provides_output_to_step\n    class component\n    dict component_parameters\n  }</code></pre> <p>As per the above, a pipeline job consists of a list of tasks. Each task consists of a list of steps. Each step consists of a component and a set of parameters that are passed to the component. Dependency Injection will ensure that each component is instantiated with the correct parameters.</p>"},{"location":"blog/rtdip_ingestion_pipelines/#pipeline-runtime-environments","title":"Pipeline Runtime Environments","text":"Python Apache Spark Databricks Delta Live Tables <p>Pipelines will be able to run in multiple environment types. These will include:</p> <ul> <li>Python: Components will be written in python and executed on a python runtime</li> <li>Pyspark: Components will be written in pyspark and executed on an open source Apache Spark runtime</li> <li>Databricks: Components will be written in pyspark and executed on a Databricks runtime</li> <li>Delta Live Tables: Components will be written in pyspark and executed on a Databricks runtime and will write to Delta Live Tables</li> </ul> <p>Runtimes will take precedence depending on the list of components in a pipeline task.</p> <ul> <li>Pipelines with at least one Databricks or DLT component will be executed in a Databricks environment</li> <li>Pipelines with at least one Pyspark component will be executed in a Pyspark environment</li> <li>Pipelines with only Python components will be executed in a Python environment</li> </ul>"},{"location":"blog/rtdip_ingestion_pipelines/#pipeline-clouds","title":"Pipeline Clouds","text":"<p>Certain components are related to cloud providers and in the tables below, it is indicated which cloud provider is related to its specific component. It does not mean that the component can only run in that cloud, instead its highlighting that the component is related to that cloud provider.</p> Cloud Target Azure Q1-Q2 2023 AWS Q2-Q4 2023 GCP 2024"},{"location":"blog/rtdip_ingestion_pipelines/#pipeline-orchestration","title":"Pipeline Orchestration","text":"Airflow Databricks Dagster <p>Pipelines will be able to be deployed to orchestration engines so that users can schedule and execute jobs using their preferred orchestration engine.</p> Orchestration Engine Target Databricks Workflows Q2 2023 Airflow Q2 2023 Delta Live Tables Q3 2023 Dagster Q4 2023"},{"location":"blog/rtdip_ingestion_pipelines/#pipeline-components","title":"Pipeline Components","text":"<p>The Real Time Data Ingestion Pipeline Framework will support the following components:</p> <ul> <li>Sources - connectors to source systems</li> <li>Transformers - perform transformations on data, including data cleansing, data enrichment, data aggregation, data masking, data encryption, data decryption, data validation, data conversion, data normalization, data de-normalization, data partitioning etc</li> <li>Destinations - connectors to sink/destination systems </li> <li>Utilities - components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance etc</li> <li>Edge - components that will perform edge functionality such as connectors to protocols like OPC</li> </ul>"},{"location":"blog/rtdip_ingestion_pipelines/#pipeline-component-types","title":"Pipeline Component Types","text":"Python Apache Spark Databricks <p>Component Types determine system requirements to execute the component:</p> <ul> <li>Python - components that are written in python and can be executed on a python runtime</li> <li>Pyspark - components that are written in pyspark can be executed on an open source Apache Spark runtime</li> <li>Databricks - components that require a Databricks runtime</li> </ul>"},{"location":"blog/rtdip_ingestion_pipelines/#sources","title":"Sources","text":"<p>Sources are components that connect to source systems and extract data from them. These will typically be real time data sources, but will also support batch components as these are still important and necessary data souces in a number of circumstances in the real world.</p> Source Type Python Apache Spark Databricks Azure AWS Target Delta * Q1 2023 Delta Sharing * Q1 2023 Autoloader Q1 2023 Eventhub * Q1 2023 IoT Hub * Q2 2023 Kafka Q2 2023 Kinesis Q2 2023 IoT Core Q2 2023 SSIP PI Connector Q2 2023 Rest API Q2 2023 MongoDB Q3 2023 <p>* - target to deliver in the following quarter</p>"},{"location":"blog/rtdip_ingestion_pipelines/#transformers","title":"Transformers","text":"<p>Transformers are components that perform transformations on data. These will target certain data models and common transformations that sources or destination components require to be performed on data before it can be ingested or consumed.</p> Transformer Type Python Apache Spark Databricks Azure AWS Target Eventhub Body Q1 2023 OPC UA Q2 2023 OPC AE Q2 2023 SSIP PI Q2 2023 OPC DA Q3 2023 <p>* - target to deliver in the following quarter</p> <p>This list will dynamically change as the framework is developed and new components are added.</p>"},{"location":"blog/rtdip_ingestion_pipelines/#destinations","title":"Destinations","text":"<p>Destinations are components that connect to sink/destination systems and write data to them. </p> Destination Type Python Apache Spark Databricks Azure AWS Target Delta Append * Q1 2023 Eventhub * Q1 2023 Delta Merge Q2 2023 Kafka Q2 2023 Kinesis Q2 2023 Rest API Q2 2023 MongoDB Q3 2023 Polygon Blockchain Q3 2023 <p>* - target to deliver in the following quarter</p>"},{"location":"blog/rtdip_ingestion_pipelines/#utilities","title":"Utilities","text":"<p>Utilities are components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance and are normally components that can be executed as part of a pipeline or standalone.</p> Utility Type Python Apache Spark Databricks Azure AWS Target Delta Table Create * Q1 2023 Delta Optimize Q2 2023 Delta Vacuum * Q2 2023 Set ADLS Gen2 ACLs Q2 2023 Set S3 ACLs Q2 2023 Great Expectations Q3 2023 <p>* - target to deliver in the following quarter</p>"},{"location":"blog/rtdip_ingestion_pipelines/#edge","title":"Edge","text":"<p>Edge components are designed to provide a lightweight, low latency, low resource consumption, data ingestion framework for edge devices. These components will be designed to run on edge devices such as Raspberry Pi, Jetson Nano, etc. For cloud providers, this will be designed to run on AWS Greengrass and Azure IoT Edge.</p> Edge Type Azure IoT Edge AWS Greengrass Target OPC CloudPublisher Q3-Q4 2023 Greengrass OPC UA Q4 2023"},{"location":"blog/rtdip_ingestion_pipelines/#conclusion","title":"Conclusion","text":"<p>This is a very high level overview of the framework and the components that will be developed. As the framework is open source, the lists defined above and timelines can change depending on circumstances and resource availability. Its an exciting year for 2023 for the Real Time Data Ingestion Platform. Check back in regularly for updates and new features! If you would like to contribute, please visit our repository on Github and connect with us on our Slack channel on the LF Energy Foundation Slack workspace.</p>"},{"location":"getting-started/about-rtdip/","title":"About RTDIP","text":""},{"location":"getting-started/about-rtdip/#rtdip-and-lf-energy","title":"RTDIP and LF Energy","text":"<p>By providing frameworks and reference implementations, LF Energy minimizes pain points such as cybersecurity, interoperability, control, automation, virtualization, and the orchestration of supply and demand.</p> <p>RTDIP is an LF Energy project and forms part of an overall open source energy ecosystem. To find out more about projects in LF Energy, please click here.</p>"},{"location":"getting-started/about-rtdip/#what-is-real-time-data-ingestion-platform","title":"What is Real Time Data Ingestion Platform","text":"<p>Organizations need data for day-to-day operations and to support advanced Data Science, Statistical and Machine Learning capabilities such as Optimization, Surveillance, Forecasting, and Predictive Analytics. Real Time Data forms a major part of the total data utilized in these activities.</p> <p>Real time data enables organizations to detect and respond to changes in their systems thus improving the efficiency of their operations. This data needs to be available in scalable and secure data platforms. </p> <p>Real Time Data Ingestion Platform (RTDIP) is the solution of choice leveraging PaaS (Platform as a Service) services along with some custom components to provide Data Ingestion, Data Transformation, and Data Sharing as a platform. RTDIP can interface with several data sources to ingest many different data types which include time series, alarms, video, photos and audio being provided from sources such as Historians, OPC Servers and Rest APIs, as well as data being sent from hardware such as IoT Sensors, Robots and Drones.</p>"},{"location":"getting-started/about-rtdip/#why-real-time-data-ingestion-platform","title":"\u200bWhy Real Time Data Ingestion Platform?","text":"<p>Real Time Data Ingestion Platform (RTDIP) enables users to consume Real Time Data at scale, including historical and real time streaming data. RTDIP has proven to be capable of ingesting over 3 million sensors in a production environment across every geographical location in the world.</p> <p>The Real Time Data Ingestion Platform can be run in a customers own environment, allowing them to accelerate their cloud deployments while leveraging a proven design to scale their time series data needs. </p> <p>RTDIP also provides a number popular integration options, including:</p> <ol> <li>ODBC</li> <li>JDBC</li> <li>Rest API</li> <li>Python SDK</li> </ol> <p>These options allow users to integrate with a wide variety of applications and tools, including:</p> <ol> <li>Data Visualization Tools such as Power BI, Seeq, Tableau, and Grafana</li> <li>Data Science Tools such as Jupyter Notebooks, R Studio, and Python</li> <li>Data Engineering Tools such as Apache Spark, Apache Kafka, and Apache Airflow</li> </ol> <p>RTDIP is architected to leverage Open Source technologies Apache Spark and Delta. This allows users to leverage the power of Open Source technologies to build their own custom applications and tools in whichever environment they prefer, whether that is in the cloud or on-premise on their own managed Spark Clusters.</p>"},{"location":"getting-started/installation/","title":"Getting started with RTDIP","text":"<p>RTDIP provides functionality to process and query real time data. The RTDIP SDK is central to building pipelines and querying data, so getting started with it is key to unlocking the capability of RTDIP.</p> <p>This article provides a guide on how to install the RTDIP SDK. Get started by ensuring you have all the prerequisites before following the simple installation steps.</p> <ul> <li> <p>Prerequisites</p> </li> <li> <p>Installation</p> </li> </ul>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#python","title":"Python","text":"<p>There are a few things to note before using the RTDIP SDK. The following prerequisites will need to be installed on your local machine.</p> <p>Python version 3.8 &gt;= and &lt; 3.11 should be installed. Check which python version you have with the following command:</p> <pre><code>python --version\n</code></pre> <p>Find the latest python version here and ensure your python path is set up correctly on your machine.</p>"},{"location":"getting-started/installation/#python-package-installers","title":"Python Package Installers","text":"<p>Installing the RTDIP can be done using a package installer, such as Pip, Conda or Micromamba.</p> PipCondaMicromamba <p>Ensure your pip python version matches the python version on your machine. Check which version of pip you have installed with the following command:</p> <pre><code>pip --version\n</code></pre> <p>There are two ways to ensure you have the correct versions installed. Either upgrade your Python and pip install or create an environment.</p> <pre><code>python -m pip install --upgrade pip\n</code></pre> <p>Check which version of Conda is installed with the following command:</p> <pre><code>conda --version\n</code></pre> <p>If necessary, upgrade Conda as follows:</p> <pre><code>conda update conda\n</code></pre> <p>Check which version of Micromamba is installed with the following command:</p> <pre><code>micromamba --version\n</code></pre> <p>If necessary, upgrade Micromamba as follows:</p> <pre><code>micromamba self-update\n</code></pre>"},{"location":"getting-started/installation/#odbc","title":"ODBC","text":"<p>To use pyodbc or turbodbc python libraries, ensure that the required ODBC driver is installed as per these instructions.</p>"},{"location":"getting-started/installation/#pyodbc","title":"Pyodbc","text":"<p>If you plan to use pyodbc, Microsoft Visual C++ 14.0 or greater is required. Get it from Microsoft C++ Build Tools.</p>"},{"location":"getting-started/installation/#turbodbc","title":"Turbodbc","text":"<p>To use turbodbc python library, ensure to follow the Turbodbc Getting Started section and ensure that Boost is installed correctly.</p>"},{"location":"getting-started/installation/#java","title":"Java","text":"<p>If you are planning to use the RTDIP Pipelines in your own environment that leverages pyspark for a component, Java 8 or later is a prerequisite. See below for suggestions to install Java in your development environment.</p> CondaJava <p>A fairly simple option is to use the conda openjdk package to install Java into your python virtual environment. An example of a conda environment.yml file to achieve this is below.</p> <pre><code>name: rtdip-sdk\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- python==3.10\n- pip==23.0.1\n- openjdk==11.0.15\n- pip:\n- rtdip-sdk\n</code></pre> <p>Pypi</p> <p>This package is not available from Pypi.</p> <p>Follow the official Java JDK installation documentation here.</p> <ul> <li>Windows</li> <li>Mac OS</li> <li>Linux</li> </ul> <p>Windows</p> <p>Windows requires an additional installation of a file called winutils.exe. Please see this repo for more information.</p>"},{"location":"getting-started/installation/#installing-the-rtdip-sdk","title":"Installing the RTDIP SDK","text":"<p>RTDIP SDK is a PyPi package that can be found here. On this page you can find the project description,  release history, statistics, project links and maintainers.</p> <p>Features of the SDK can be installed using different extras statements when installing the rtdip-sdk package:</p> QueriesPipelinesPipelines + Pyspark <p>When installing the package for only quering data, simply specify  in your preferred python package installer:</p> <pre><code>rtdip-sdk\n</code></pre> <p>RTDIP SDK can be installed to include the packages required to build, execute and deploy pipelines. Specify the following extra [pipelines] when installing RTDIP SDK so that the required python packages are included during installation.</p> <pre><code>rtdip-sdk[pipelines]\n</code></pre> <p>RTDIP SDK can also execute pyspark functions as a part of the pipelines functionality. Specify the following extra [pipelines,pyspark] when installing RTDIP SDK so that the required pyspark python packages are included during installation.</p> <pre><code>rtdip-sdk[pipelines,pyspark]\n</code></pre> <p>Java</p> <p>Ensure that Java is installed prior to installing the rtdip-sdk with the [pipelines,pyspark]. See here for more information.</p> <p>The following provides examples of how to install the RTDIP SDK package with Pip, Conda or Micromamba. Please note the section above to update any extra packages to be installed as part of the RTDIP SDK.</p> PipCondaMicromamba <p>To install the latest released version of RTDIP SDK from PyPi use the following command:</p> <pre><code>pip install rtdip-sdk\n</code></pre> <p>If you have previously installed the RTDIP SDK and would like the latest version, see below:</p> <pre><code>pip install rtdip-sdk --upgrade\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- python==3.10\n- pip==23.0.1\n- pip:\n- rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>conda env update -f environment.yml\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- python==3.10\n- pip==23.0.1\n- pip:\n- rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>micromamba create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>micromamba update -f environment.yml\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next steps","text":"<p>Once the installation is complete you can learn how to use the SDK here.</p> <p>Note</p> <p>If you are having trouble installing the SDK, ensure you have installed all of the prerequisites. If problems persist please see Troubleshooting for more information. Please also reach out to the RTDIP team via Issues, we are always looking to improve the SDK and value your input.</p>"},{"location":"integration/power-bi/","title":"Integration of Power BI with RTDIP","text":""},{"location":"integration/power-bi/#integration-with-power-bi","title":"Integration with Power BI","text":"<p>Microsoft Power BI is a business analytics service that provides interactive visualizations with self-service business intelligence capabilities that enable end users to create reports and dashboards by themselves without having to depend on information technology staff or database administrators.</p> <p></p> <p>When you use Azure Databricks as a data source with Power BI, you can bring the advantages of Azure Databricks performance and technology beyond data scientists and data engineers to all business users.</p> <p>You can connect Power BI Desktop to your Azure Databricks clusters and Databricks SQL warehouses by using the built-in Azure Databricks connector. You can also publish Power BI reports to the Power BI service and enable users to access the underlying Azure Databricks data using single sign-on (SSO), passing along the same Azure Active Directory credentials they use to access the report.</p> <p>For more information on how to connect Power BI with databricks, see here.</p>"},{"location":"integration/power-bi/#power-bi-installation-instructions","title":"Power BI Installation Instructions","text":"<ol> <li> <p>Install Power BI Desktop application from Microsoft Store using your Microsoft Account to sign in. </p> </li> <li> <p>Open Power BI desktop.</p> </li> <li> <p>Click on Home, Get data and More... </p> </li> <li> <p>Search for Azure Databricks and click Connect.  </p> </li> <li> <p>Fill in the details and click OK.</p> </li> <li> <p>Connect to the RTDIP data using your Databricks SQL Warehouse connection details including Hostname and HTTP Path. For Data Connectivity mode, select DirectQuery.</p> </li> <li> <p>Click Azure Active Directory, Sign In and select Connect. In Power Query Editor, there are different tables for different data types. </p> </li> <li> <p>Once connected to the Databricks SQL Warehouse, navigate to the Business Unit in the navigator bar on the left and select the asset tables for the data you wish to use in your report. There is functionality to select multiple tables if required. Click Load to get the queried data.</p> </li> </ol>"},{"location":"releases/core/","title":"Releases","text":""},{"location":"releases/core/#v020","title":"v0.2.0What's ChangedNew Contributors","text":"<p>v0.2.0  Published 2023-04-03 12:32:58 </p> Bug Fixes <ul> <li>Fix for docker build error by @GBBBAS in https://github.com/rtdip/core/pull/114 </li> <li>Fix for docker build by @GBBBAS in https://github.com/rtdip/core/pull/115 </li> <li>Time weighted average bug fix by @cching95 in https://github.com/rtdip/core/pull/120 </li> </ul> Other Changes <ul> <li>Documentation: Add Releases Feed to Docs Release page by @GBBBAS in https://github.com/rtdip/core/pull/69 </li> <li>Documentation: Update Packages and Doc Logos by @GBBBAS in https://github.com/rtdip/core/pull/76 </li> <li>Documentation: Fix Logo Alignment by @GBBBAS in https://github.com/rtdip/core/pull/78 </li> <li>Pipelines: Upgrade dbx by @GBBBAS in https://github.com/rtdip/core/pull/79 </li> <li>Pipelines: add spark session by @cching95 in https://github.com/rtdip/core/pull/80 </li> <li>Pipelines: Add Pipelines as Extras libraries to setup by @GBBBAS in https://github.com/rtdip/core/pull/82 </li> <li>Pipelines: Spark pipeline source Event hub and destination Event hub by @cching95 in https://github.com/rtdip/core/pull/83 </li> <li>Pipelines: Spark pipeline Source and Destination Delta Components by @rodalynbarce in https://github.com/rtdip/core/pull/84 </li> <li>Pipelines: Added Source Delta Sharing component with docs by @JamesKnBr in https://github.com/rtdip/core/pull/85 </li> <li>Pipelines: Unit Tests for sources by @cching95 in https://github.com/rtdip/core/pull/88 </li> <li>Pipelines: Add Transformer Eventhub body transformer by @cching95 in https://github.com/rtdip/core/pull/89 </li> <li>Pipelines: Added pipeline source autoloader and unit test by @rodalynbarce in https://github.com/rtdip/core/pull/93 </li> <li>Pipelines: Pipeline Executor by @GBBBAS in https://github.com/rtdip/core/pull/95 </li> <li>Pipelines: Delta Table Create Utility by @GBBBAS in https://github.com/rtdip/core/pull/97 </li> <li>Pipelines: Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/102 </li> <li>Pipelines: Pipeline Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/103 </li> <li>Pipelines: Updates for Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/104 </li> <li>Pipelines: Unit test for spark pipeline eventhub destination by @cching95 in https://github.com/rtdip/core/pull/105 </li> <li>Pipelines: Dependency Injector on Pipeline Execute by @GBBBAS in https://github.com/rtdip/core/pull/106 </li> <li>Pipelines: Unit tests for source and destination streams by @cching95 in https://github.com/rtdip/core/pull/108 </li> <li>Pipelines: Pipeline Deployments and Pipeline Secrets by @GBBBAS in https://github.com/rtdip/core/pull/111 </li> <li>Pipelines: Added fails tests to pipeline sources by @JamesKnBr in https://github.com/rtdip/core/pull/112 </li> <li>Containers: Dockerfile refactor by @GBBBAS in https://github.com/rtdip/core/pull/116 </li> <li>Pipelines: Pipeline Secrets, Converters and Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/119 </li> </ul> <ul> <li>@rodalynbarce made their first contribution in https://github.com/rtdip/core/pull/84 </li> <li>@JamesKnBr made their first contribution in https://github.com/rtdip/core/pull/85 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/commits/v0.2.0</p>"},{"location":"releases/core/#v020-beta1","title":"v0.2.0-beta.1What's ChangedNew Contributors","text":"<p>v0.2.0-beta.1  Published 2023-03-30 10:21:49  Pre-release </p> Bug Fixes <ul> <li>Hotfix to remove print statement in twa function by @cching95 in https://github.com/rtdip/core/pull/99 </li> <li>Fix for docker build error by @GBBBAS in https://github.com/rtdip/core/pull/114 </li> <li>Fix for docker build by @GBBBAS in https://github.com/rtdip/core/pull/115 </li> <li>time weighted average bug fix by @cching95 in https://github.com/rtdip/core/pull/120 </li> </ul> Ingestion Framework <ul> <li>Spark pipeline source Event hub and destination Event hub by @cching95 in https://github.com/rtdip/core/pull/83 </li> <li>Spark pipeline source delta and destination delta by @rodalynbarce in https://github.com/rtdip/core/pull/84 </li> <li>Added delta sharing with docs by @JamesKnBr in https://github.com/rtdip/core/pull/85 </li> <li>Unit Tests for sources by @cching95 in https://github.com/rtdip/core/pull/88 </li> <li>Added pipeline source autoloader and unit test by @rodalynbarce in https://github.com/rtdip/core/pull/93 </li> <li>Pipeline Executor by @GBBBAS in https://github.com/rtdip/core/pull/95 </li> <li>Delta Table Create Utility by @GBBBAS in https://github.com/rtdip/core/pull/97 </li> <li>Upgrade dbx by @GBBBAS in https://github.com/rtdip/core/pull/79 </li> <li>add spark session by @cching95 in https://github.com/rtdip/core/pull/80 </li> <li>Add Pipelines as Extras libraries to setup by @GBBBAS in https://github.com/rtdip/core/pull/82 </li> <li>Add pipeline eventhub body transformer by @cching95 in https://github.com/rtdip/core/pull/89 </li> <li>Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/102 </li> <li>Pipeline Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/103 </li> <li>Updates for Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/104 </li> <li>Unit test for spark pipeline eventhub destination by @cching95 in https://github.com/rtdip/core/pull/105 </li> <li>Dependency Injector on Pipeline Execute by @GBBBAS in https://github.com/rtdip/core/pull/106 </li> <li>Unit tests for source and destination streams by @cching95 in https://github.com/rtdip/core/pull/108 </li> <li>Pipeline Deployments and Pipeline Secrets by @GBBBAS in https://github.com/rtdip/core/pull/111 </li> <li>Added fails tests to pipeline sources by @JamesKnBr in https://github.com/rtdip/core/pull/112 </li> <li>Pipeline Secrets, Converters and Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/119 </li> </ul> Documentation <ul> <li>Add Releases Feed to Docs Release page by @GBBBAS in https://github.com/rtdip/core/pull/69 </li> <li>Update Packages and Doc Logos by @GBBBAS in https://github.com/rtdip/core/pull/76 </li> <li>Fix Logo Alignment by @GBBBAS in https://github.com/rtdip/core/pull/78 </li> </ul> Other Changes <ul> <li>Dockerfile refactor by @GBBBAS in https://github.com/rtdip/core/pull/116 </li> <li>Update python version for Release build by @GBBBAS in https://github.com/rtdip/core/pull/123 </li> </ul> <ul> <li>@rodalynbarce made their first contribution in https://github.com/rtdip/core/pull/84 </li> <li>@JamesKnBr made their first contribution in https://github.com/rtdip/core/pull/85 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/compare/v0.1.6...v0.2.0</p>"},{"location":"releases/core/#v016","title":"v0.1.6What's Changed","text":"<p>v0.1.6  Published 2023-03-21 11:41:08 </p> Other Changes <ul> <li>Remove print statement in TWA function by @cching95 in https://github.com/rtdip/core/pull/100 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/compare/v0.1.5...v0.1.6</p>"},{"location":"releases/core/#v015","title":"v0.1.5What's Changed","text":"<p>v0.1.5  Published 2023-02-24 16:55:17 </p> Other Changes <ul> <li>Add OS Matrix to CI Tests by @GBBBAS in https://github.com/rtdip/core/pull/65 </li> <li>Real Time Data Pipelines Blog by @GBBBAS in https://github.com/rtdip/core/pull/60 </li> <li>Remove OS matrix from Actions due to Boost by @GBBBAS in https://github.com/rtdip/core/pull/66 </li> <li>v0.1.5 by @GBBBAS in https://github.com/rtdip/core/pull/67 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/compare/v0.1.4...v0.1.5</p>"},{"location":"releases/core/#v014","title":"v0.1.4What's Changed","text":"<p>v0.1.4  Published 2023-02-09 09:23:43 </p> Bug Fixes <ul> <li>Removal of packaging legacy version dependency by @NooraKubati in https://github.com/rtdip/core/pull/57 </li> </ul> Other Changes <ul> <li>Update tests for multiple versions of Python by @GBBBAS in https://github.com/rtdip/core/pull/40 </li> <li>Roadmap 2023 by @GBBBAS in https://github.com/rtdip/core/pull/45 </li> <li>v0.1.4 by @GBBBAS in https://github.com/rtdip/core/pull/46 </li> <li>updated devcontainer.json by @cching95 in https://github.com/rtdip/core/pull/50 </li> <li>fix spelling error in releases document by @cching95 in https://github.com/rtdip/core/pull/53 </li> <li>v0.1.4 #2 by @NooraKubati in https://github.com/rtdip/core/pull/58 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/compare/v0.1.3...v0.1.4</p>"},{"location":"releases/core/#v013","title":"v0.1.3What's ChangedNew Contributors","text":"<p>v0.1.3  Published 2022-12-14 11:11:41 </p> Other Changes <ul> <li>Add CNAME file to docs for custom domain www.rtdip.io by @GBBBAS in https://github.com/rtdip/core/pull/34 </li> <li>API for Time weighted average by @cching95 in https://github.com/rtdip/core/pull/29 </li> <li>Upgrade Python to 3.11 by @GBBBAS in https://github.com/rtdip/core/pull/36 </li> <li>v0.1.3 by @NooraKubati in https://github.com/rtdip/core/pull/38 </li> </ul> <ul> <li>@cching95 made their first contribution in https://github.com/rtdip/core/pull/29 </li> <li>@NooraKubati made their first contribution in https://github.com/rtdip/core/pull/38 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/compare/v0.1.2...v0.1.3</p>"},{"location":"releases/core/#v012","title":"v0.1.2What's ChangedNew Contributors","text":"<p>v0.1.2  Published 2022-12-08 15:07:23 </p> New Features <ul> <li>Updates for Mamba in CI/CD by @GBBBAS in https://github.com/rtdip/core/pull/21 </li> </ul> Other Changes <ul> <li>Add Mamba for Conda Installation by @GBBBAS in https://github.com/rtdip/core/pull/15 </li> <li>Update to Micromamba Config in Github Actions by @GBBBAS in https://github.com/rtdip/core/pull/22 </li> <li>Change to Micromamba Config by @GBBBAS in https://github.com/rtdip/core/pull/23 </li> <li>Update scopes and remove Static App Configuration by @GBBBAS in https://github.com/rtdip/core/pull/20 </li> <li>Add badges to Readme by @GBBBAS in https://github.com/rtdip/core/pull/17 </li> <li>Add VSCode devcontainer by @GBBBAS in https://github.com/rtdip/core/pull/25 </li> <li>Update to prerelease of docker containers by @GBBBAS in https://github.com/rtdip/core/pull/28 </li> <li>Add RTDIP LF Energy logos by @GBBBAS in https://github.com/rtdip/core/pull/31 </li> <li>v0.1.2 by @Amber-Rigg in https://github.com/rtdip/core/pull/32 </li> </ul> <ul> <li>@Amber-Rigg made their first contribution in https://github.com/rtdip/core/pull/32 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/compare/v0.1.1...v0.1.2</p>"},{"location":"releases/core/#v011","title":"v0.1.1What's ChangedNew Contributors","text":"<p>v0.1.1  Published 2022-11-01 18:58:45 </p> Bug Fixes <ul> <li>Fix to Mkdocs package by @GBBBAS in https://github.com/rtdip/core/pull/7 </li> </ul> Other Changes <ul> <li>Configure Mend Bolt for GitHub by @mend-bolt-for-github in https://github.com/rtdip/core/pull/3 </li> <li>v0.1.1 by @GBBBAS in https://github.com/rtdip/core/pull/8 </li> <li>Fix to Mkdocs String Package by @GBBBAS in https://github.com/rtdip/core/pull/12 </li> <li>v0.1.1 by @GBBBAS in https://github.com/rtdip/core/pull/13 </li> </ul> <ul> <li>@mend-bolt-for-github made their first contribution in https://github.com/rtdip/core/pull/3 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/compare/v0.1.0...v0.1.1</p>"},{"location":"releases/core/#v010","title":"v0.1.0What's ChangedNew Contributors","text":"<p>v0.1.0  Published 2022-11-01 17:47:16 </p> Other Changes <ul> <li>Real Time Data Ingestion Platform by @GBBBAS in https://github.com/rtdip/core/pull/2 </li> <li>v0.1.0 by @GBBBAS in https://github.com/rtdip/core/pull/4 </li> </ul> <ul> <li>@GBBBAS made their first contribution in https://github.com/rtdip/core/pull/2 </li> </ul> <p>Full Changelog: https://github.com/rtdip/core/commits/v0.1.0</p>"},{"location":"roadmap/roadmap-overview/","title":"RTDIP Roadmap","text":"<p>This section provides periodical updates of what the RTDIP team will be working on over the short and long term. The team will provide updates at important periods of the year so that users of the platform can understand new capabilities and options coming to the platform.</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP team if you have ideas or suggestions on what we could work on next.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/","title":"RTDIP Development Roadmap in 2022","text":"<p>Defining a list of development items for RTDIP is always difficult because so much can change within  Digital Technologies in 12 months. However, as we head towards the end of the year of 2021, we have outlined themes of what RTDIP will look at in the Development and Innovation space in 2022.</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP Technical Steering Committee team if you have ideas or suggestions on what we could innovate on in this space. We will continue to evolve these development items all through 2022 so please come back to this article throughout the year to see any new items that may be brought into scope.</p> <p>Note</p> <p>Development and Innovation items don't always make it to Production. </p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#tldr","title":"TL;DR","text":"<p>A brief summary of development and innovation items planned for 2022.</p> Item Description Estimated Quarter for Delivery Power BI Enable querying of RTDIP data via Power BI. While some work has started on this in 2021, this item explores rolling it out further and how users can combine RTDIP data with other data sources Q1 2022 Seeq Connector Enable querying of RTDIP data via Seeq. Scope is limited to simply querying RTDIP data, we may look at what else is possible with the connector once the base capability has been achieved Q1 2022 Delta Live Tables Leverage Delta Live Tables for ingestion of RTDIP data into Delta Format. Provides better processing, merging, data cleansing and monitoring capabilities to the RTDIP Delta Ingestion Pipelines Q1-Q2 2022 Multicloud Build certain existing RTDIP Azure capabilities on AWS. Enables RTDIP in the clouds aligned with the business but also to ensure multicloud is cost effective and that products in the architecture work in Cloud Environments Q1-Q3 2022 SDK An open source python SDK is developed to assist users with a simple python library for connecting, authenticating and querying RTDIP data Q1-Q4 2022 REST API Wrap the python SDK in a REST API to allow non Python users to get similar functionality to the python SDK Q1-Q4 2022 Unity Catalog Provides a multi-region Catalog of all data in RTDIP. Enables easier navigation and exploration of what datasets are available in RTDIP Q3 2022 Delta Sharing Enables sharing of Delta data via a managed service that handles security, authentication and delivery of data. Particularly useful for sharing RTDIP data with third parties Q4 2022"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#power-bi","title":"Power BI","text":"<p>Power BI is a popular tool amongst RTDIP End Users for querying and plotting RTDIP data. The use of Delta and Databricks SQL Warehouses in the RTDIP Platform brings native Power BI integration using connectors already available in Power BI versions after May 2021.</p> <p>The aim is to enable Power BI connectivity to RTDIP so that users can query their data by the end of Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#seeq","title":"Seeq","text":"<p>Similar to Power BI, Seeq is a popular tool amongst real time users to query and manipulate RTDIP data. Seeq and RTDIP are currently working on a connector that allows Seeq to query RTDIP data via the same Databricks SQL Warehouse that Power BI will use for querying data by the end of Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#delta-live-tables","title":"Delta Live Tables","text":"<p>For more information about the advantages of Delta Live Tables, please see this link and if you would like to see Bryce Bartmann, RTDIP team member, talking about Delta Live Tables at the Data &amp; AI Summit 2021, please see the session here.</p> <p>RTDIP has been converting it's data to the open source format Delta using standard PySpark structured streaming jobs. Whilst this has been working well for converting RTDIP data to Delta, Delta Live Tables from Databricks provides similar capabilities as standard spark code, but with additional benefits:</p> <ul> <li>Expectations: Allows developers to specify data cleansing rules on ingested data. This can assist to provide higher quality, more reliable data to users</li> <li>Data Flows: Visually describes the flow of the data through the data pipelines from source to target, including data schemas and record counts</li> <li>Maintenance: Delta Live Tables simplifies maintenance tasks required for Delta Tables by scheduling them automatically based on the deployment of the Delta Live Tables Job</li> <li>Monitoring: Delta Live Tables are easier to monitor as their graphical schematics help non-technical people to understand the status of the ingestion pipelines</li> </ul> <p>The RTDIP Team has actively worked with Databricks to build Delta Live Tables. Whilst the product is well understood, certain features like merging data needed to be made available before RTDIP could fully migrate existing spark jobs to Delta Live Tables. Databricks intend to provide the Merge function in late Q4 2021 which will then trigger this piece of work with a target of having a decision point to move to production in Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#multicloud","title":"Multicloud","text":"<p>As clouds mature, one of the most asked questions is how customers can leverage more than one cloud to provide a better and more economical solution to their customers. Even though this is a fairly new area to explore, there are a number of cloud agnostic technologies that are trying to help customers take advantage of and manage environments in more than one cloud.</p> <p>Multicloud design can be complex and requires significant analysis of existing technologies capabilities and how they translate into benefits for RTDIP customers. Databricks will be one good example of exploring their new multicloud environment management tool and how this could benefit businesses in the long run. We expect there to be more technologies that come out with multicloud capabilities throughout 2022 and we will continue to explore, test and understand how RTDIP can leverage these throughout 2022 and beyond.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#sdk-and-rest-api","title":"SDK and REST API","text":"<p>A common theme we are seeing amongst applications and users of RTDIP data is a simple way to authenticate, query and manipulate RTDIP data. In an effort to also build a stronger developer community around RTDIP, we will be building a python SDK that python users can use in their code for performing common functions with RTDIP Data:</p> <ul> <li>Authenticating with RTDIP</li> <li>Connecting to RTDIP data</li> <li>Querying RTDIP raw data</li> <li>Performing sampling on raw data</li> <li>Performing interpolation on sampled data</li> </ul> <p>We plan to deliver the first version of the python SDK early in 2022 and welcome all python developers to contribute to the repository. </p> <p>For non python users, we plan to wrap the SDK in a REST API. This facilitates a language agnostic way of benefitting from all the development of the python SDK. These REST APIs will be rolled out in line with functionality built with the python SDK.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#unity-catalog","title":"Unity Catalog","text":"<p>Cataloging data is a common activity when building data lakes that contain data from multiple sources and from multiple geographic regions. RTDIP will explore and deploy a catalog of all data sources currently being ingested into the platform.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#delta-sharing","title":"Delta Sharing","text":"<p>One of the most common requests the RTDIP team receive is how to share RTDIP data with third parties. Delta Sharing is an open source capability that allows sharing of Delta data via a managed service that provides authentication, connection management and supply of Delta data to third parties. </p> <p>We aim to see what more we can do in this space to make sharing of data simpler from an architecture perspective while still meeting all the security requirements around sharing of data with third parties.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/","title":"RTDIP Development Roadmap in 2023","text":"<p>Shell\u2019s internally developed Shell Sensor Intelligence Platform was open sourced to LF Energy in 2022 and rebranded to Real Time Data Ingestion Platform. This was a major milestone for that project and has opened a number of strategic doors and expanded how the development roadmap is determined when considering it in an open source context. Below is the roadmap for 2023</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP Technical Steering Committee team if you have ideas or suggestions on what we could innovate on in this space. We will continue to evolve these development items all through 2022 so please come back to this article throughout the year to see any new items that may be brought into scope.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#tldr","title":"TL;DR","text":"Item Description Estimated Quarter for Delivery IEC CIM RTDIP will target the IEC CIM data model and focus specifically on modelling data related to scada/process time series and sensor metadata Q1 2023 Ingestion Framework Develop a Data Ingestion framework, incorporated into the RTDIP SDK that supports building, testing and deploying streaming data pipelines into an RTDIP environment. The goal of this framework is to eventually facilitate low-code/no-code setup of streaming data pipelines that can be easily deployed by RTDIP users. In 2023, the first iteration of the framework will be designed and developed Q1-Q2 2023 Meter Data Design and build ingestion pipelines for meter data that meets the IEC CIM and Data Ingestion Framework items above Q2-Q4 2023 LF Energy Integration Increase integration of RTDIP with other LF Energy components. RTDIP will reach out to LF Energy projects to understand which data and integration opportunities exist and support building the necessary components to support these integrations Q1-Q4 2023 Microgrid Support Deployment of RTDIP and related LF Energy components to a pilot microgrid site Q1-Q4 2023 Multicloud Build certain existing RTDIP Azure capabilities on AWS. Enables RTDIP in the clouds aligned with the business but also to ensure multicloud is cost effective and that products in the architecture work in Cloud Environments Q1-Q3 2023 Adoption Adapt RTDIP to cater for a wider audience, particularly to increase RTDIP adoption in the energy sector. A simpler deployment process, facilitation of more environments RTDIP can setup in and catering for more orchestration engines are all initiatives to increase adoption in 2023 Q1-Q4 2023"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#iec-cim","title":"IEC CIM","text":"<p>RTDIP will define IEC CIM compliant data models that will be built in the RTDIP SDK for time series data and metering data. This will ensure that RTDIP can support systems that require data modelled to the IEC CIM standard. The data models will be defined and available within the RTDIP SDK so that it can be used in the ingestion and query layers of the RTDIP SDK.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#ingestion-framework","title":"Ingestion Framework","text":"<p>An ingestion framework will be incorporated into the RTDIP SDK that will facilitate the following: - Simple reuse of components - Standardisation of development of ingestion source, transformation and destination components - A common deployment pattern that supports the most popular spark environment setups - Targeting of popular orchestration engines and targeting Databricks Worflows and Airflow. Dagster/Flyte to be considered.</p> <p>The ingestion framework will be designed and developed in Q1-Q3 2023 and will be used to build the ingestion pipelines for meter data in Q2-Q4 2023.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#meter-data","title":"Meter Data","text":"<p>RTDIP will be extended to incorporate meter data as part of the platform. The ingestion framework above will be used to build ingestion pipelines for meter data. The pipelines will be designed to ingest meter data from a variety of meter data sources and will be designed to support the IEC CIM data model.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#lf-energy-integration","title":"LF Energy Integration","text":"<p>Increase integration of RTDIP with other LF Energy components. Data is at the centre of every component in an energy system and a data platform can play a major role in supporting integration across different components. RTDIP will reach out to LF Energy projects to understand which data and integration opportunities exist and support building the necessary components to support this integration. The scope of this will be more clearly defined after connecting with the relevant projects of LF Energy to define opportunities for integration.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#microgrid-support","title":"Microgrid Support","text":"<p>Deployment of RTDIP and related LF Energy components to a Shell pilot microgrid site. This exercise will help to identify how RTDIP supports an energy system better and demonstrate the integration of RTDIP with other LF Energy components in a real world environment.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#adoption","title":"Adoption","text":"<p>Adapt RTDIP to cater for a wider audience, particularly to increase RTDIP adoption in the energy sector. To do this, the roadmap for 2023 will focus on:</p> <ul> <li>Making it simpler for deploying and incorporating RTDIP into existing technology stacks</li> <li>Provide options for deploying RTDIP ingestion jobs in different Spark Environments. For 2023, this will be Databricks(Jobs with stretch target of DLT) and self-managed Spark Clusters setup using Open Source Spark</li> <li>Support deployment of RTDIP data pipelines to different workflow managers. For 2023, this will include Airflow and Databricks Workflows, with a stretch target of including Dagster</li> <li>Support more clouds. For 2023, the target is AWS.</li> </ul>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#multicloud","title":"Multicloud","text":"<p>The target for 2023 is to enable RTDIP components to run in AWS. This includes:</p> <ul> <li>RTDIP API support on AWS Lambda</li> <li>Ingestion support on AWS Databricks</li> <li>BI support on AWS Databricks SQL</li> </ul>"},{"location":"sdk/overview/","title":"Overview","text":""},{"location":"sdk/overview/#what-is-the-rtdip-sdk","title":"What is the RTDIP SDK?","text":"<p>\u200b\u200bReal Time Data Ingestion Platform (RTDIP) SDK is a python software development kit built to provide users, data scientists and developers with the ability to interact with components of the Real Time Data Ingestion Platform, including:</p> <ul> <li>Building, Executing and Deploying Ingestion Pipelines</li> <li>Execution of queries on RTDIP data</li> <li>Authentication to securely interact with environments and data</li> </ul>"},{"location":"sdk/overview/#installation","title":"Installation","text":"<p>To get started with the RTDIP SDK, follow these installation instructions.</p>"},{"location":"sdk/authentication/azure/","title":"Azure Active Directory","text":"<p>The RTDIP SDK includes several Azure AD authentication methods to cater to the preference of the user:</p> <ul> <li>Default Authentication - For authenticating users with Azure AD using the azure-identity package. Note the order that Default Authentication uses to sign in a user and how it does it in this documentation. From experience, the Visual Studio Code login is the easiest to setup, but the azure cli option is the most reliable option. This page is useful for troubleshooting issues with this option to authenticate.</li> </ul> <p>Visual Studio Code</p> <p>As per the guidance in the documentation - To authenticate in Visual Studio Code, ensure version 0.9.11 or earlier of the Azure Account extension is installed. To track progress toward supporting newer extension versions, see this GitHub issue. Once installed, open the Command Palette and run the Azure: Sign In command.</p> <ul> <li> <p>Certificate Authentication - Service Principal authentication using a certificate</p> </li> <li> <p>Client Secret Authentication - Service Principal authentication using a client id and client secret</p> </li> </ul>"},{"location":"sdk/authentication/azure/#authentication","title":"Authentication","text":"<p>The following section describes authentication using Azure Active Directory..</p> <p>Note<p>If you are using the SDK directly in Databricks please note that DefaultAuth will not work.</p> </p> <p>1. Import rtdip-sdk authentication methods with the following:</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n</code></pre> <p>2. Use any of the following authentication methods. Replace tenant_id , client_id, certificate_path or client_secret with your own details.</p> Default AuthenticationCertificate AuthenticationClient Secret Authentication <pre><code>DefaultAzureCredential = auth.DefaultAuth().authenticate()\n</code></pre> <pre><code>CertificateCredential = auth.CertificateAuth(tenant_id, client_id, certificate_path).authenticate()\n</code></pre> <pre><code>ClientSecretCredential = auth.ClientSecretAuth(tenant_id, client_id, client_secret).authenticate()\n</code></pre> <p>3. The methods above will return back a Client Object. The following example will show you how to retrieve the access_token from a credential object. The access token will be used in later steps to connect to RTDIP via the three options (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect).</p>"},{"location":"sdk/authentication/azure/#tokens","title":"Tokens","text":"<p>Once authenticated, it is possible to retrieve tokens for specific Azure Resources by providing scopes when retrieving tokens. Please see below for examples of how to retrieve tokens for Azure resources regularly used in RTDIP.</p> Databricks <pre><code>access_token = DefaultAzureCredential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>RTDIP are continuously adding more to this list so check back regularly!</p>"},{"location":"sdk/authentication/databricks/","title":"Databricks","text":"<p>Databricks supports authentication using Personal Access Tokens (PAT) and information about this authentication method is available here.</p>"},{"location":"sdk/authentication/databricks/#authentication","title":"Authentication","text":"<p>To generate a Databricks PAT Token, follow this guide and ensure that the token is stored securely and is never used directly in code.</p> <p>Your Databricks PAT Token can be used in the RTDIP SDK to authenticate with any Databricks Workspace or Databricks SQL Warehouse and simply provided in the <code>access_token</code> fields where tokens are required in the RTDIP SDK.</p>"},{"location":"sdk/authentication/databricks/#example","title":"Example","text":"<p>Below is an example of using a Databricks PAT Token for authenticating with a Databricks SQL Warehouse.</p> <pre><code>from rtdip_sdk.odbc import db_sql_connector\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"dbapi.......\"\n\nconnection = db_sql_connector.DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path with your own information and specify your Databricks PAT token for the access_token. </p>"},{"location":"sdk/code-reference/authentication/azure/","title":"Authentication","text":""},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.authenticate.CertificateAuth","title":"<code>CertificateAuth</code>","text":"<p>Enables authentication to Azure Active Directory using a certificate that was generated for an App Registration.</p> <p>The certificate must have an RSA private key, because this credential signs assertions using RS256</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>str</code> <p>The Azure Active Directory tenant (directory) Id of the service principal.</p> required <code>client_id</code> <code>str</code> <p>The client (application) ID of the service principal</p> required <code>certificate_path</code> <code>str</code> <p>Optional path to a certificate file in PEM or PKCS12 format, including the private key. If not provided, certificate_data is required.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/authentication/authenticate.py</code> <pre><code>class CertificateAuth:\n\"\"\"\n    Enables authentication to Azure Active Directory using a certificate that was generated for an App Registration.\n\n    The certificate must have an RSA private key, because this credential signs assertions using RS256\n\n    Args:\n        tenant_id: The Azure Active Directory tenant (directory) Id of the service principal.\n        client_id: The client (application) ID of the service principal\n        certificate_path: Optional path to a certificate file in PEM or PKCS12 format, including the private key. If not provided, certificate_data is required.\n    \"\"\"\n    def __init__(self, tenant_id: str, client_id: str, certificate_path:str=None) -&gt; None:\n        self.tenant_id=tenant_id\n        self.client_id=client_id\n        self.certificate_path=certificate_path\n\n    def authenticate(self) -&gt; CertificateCredential:\n\"\"\"\n        Authenticates as a service principal using a certificate.\n\n        Returns:\n            CertificateCredential: Authenticates as a service principal using a certificate.\n        \"\"\"\n        try:   \n            access_token = CertificateCredential(self.tenant_id, self.client_id, self.certificate_path)\n            return access_token\n        except Exception as e:\n            logging.exception('error returning certificate credential')\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.authenticate.CertificateAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>Authenticates as a service principal using a certificate.</p> <p>Returns:</p> Name Type Description <code>CertificateCredential</code> <code>CertificateCredential</code> <p>Authenticates as a service principal using a certificate.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/authenticate.py</code> <pre><code>def authenticate(self) -&gt; CertificateCredential:\n\"\"\"\n    Authenticates as a service principal using a certificate.\n\n    Returns:\n        CertificateCredential: Authenticates as a service principal using a certificate.\n    \"\"\"\n    try:   \n        access_token = CertificateCredential(self.tenant_id, self.client_id, self.certificate_path)\n        return access_token\n    except Exception as e:\n        logging.exception('error returning certificate credential')\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.authenticate.ClientSecretAuth","title":"<code>ClientSecretAuth</code>","text":"<p>Enables authentication to Azure Active Directory using a client secret that was generated for an App Registration.</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>str</code> <p>The Azure Active Directory tenant (directory) Id of the service principal.</p> required <code>client_id</code> <code>str</code> <p>The client (application) ID of the service principal</p> required <code>client_secret</code> <code>str</code> <p>A client secret that was generated for the App Registration used to authenticate the client.</p> required Source code in <code>src/sdk/python/rtdip_sdk/authentication/authenticate.py</code> <pre><code>class ClientSecretAuth:\n\"\"\"\n    Enables authentication to Azure Active Directory using a client secret that was generated for an App Registration.\n\n    Args:\n        tenant_id: The Azure Active Directory tenant (directory) Id of the service principal.\n        client_id: The client (application) ID of the service principal\n        client_secret: A client secret that was generated for the App Registration used to authenticate the client.\n    \"\"\"\n    def __init__(self, tenant_id: str, client_id: str, client_secret: str) -&gt; None:\n        self.tenant_id=tenant_id\n        self.client_id=client_id\n        self.client_secret=client_secret\n\n    def authenticate(self) -&gt; ClientSecretCredential:   \n\"\"\"\n        Authenticates as a service principal using a client secret. \n\n        Returns:\n            ClientSecretCredential: Authenticates as a service principal using a client secret.\n        \"\"\"     \n        try:\n            access_token = ClientSecretCredential(self.tenant_id, self.client_id, self.client_secret)\n            return access_token\n        except Exception as e:\n            logging.exception('error returning client secret credential')\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.authenticate.ClientSecretAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>Authenticates as a service principal using a client secret. </p> <p>Returns:</p> Name Type Description <code>ClientSecretCredential</code> <code>ClientSecretCredential</code> <p>Authenticates as a service principal using a client secret.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/authenticate.py</code> <pre><code>def authenticate(self) -&gt; ClientSecretCredential:   \n\"\"\"\n    Authenticates as a service principal using a client secret. \n\n    Returns:\n        ClientSecretCredential: Authenticates as a service principal using a client secret.\n    \"\"\"     \n    try:\n        access_token = ClientSecretCredential(self.tenant_id, self.client_id, self.client_secret)\n        return access_token\n    except Exception as e:\n        logging.exception('error returning client secret credential')\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.authenticate.DefaultAuth","title":"<code>DefaultAuth</code>","text":"<p>A default credential capable of handling most Azure SDK authentication scenarios.</p> <p>The identity it uses depends on the environment. When an access token is needed, it requests one using these identities in turn, stopping when one provides a token:</p> <p>1) A service principal configured by environment variables.</p> <p>2) An Azure managed identity.</p> <p>3) On Windows only: a user who has signed in with a Microsoft application, such as Visual Studio. If multiple identities are in the cache, then the value of the environment variable AZURE_USERNAME is used to select which identity to use.</p> <p>4) The user currently signed in to Visual Studio Code.</p> <p>5) The identity currently logged in to the Azure CLI.</p> <p>6) The identity currently logged in to Azure PowerShell.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_cli_credential</code> <code>Optional</code> <p>Whether to exclude the Azure CLI from the credential. Defaults to False.</p> <code>False</code> <code>exclude_environment_credential</code> <code>Optional</code> <p>Whether to exclude a service principal configured by environment variables from the credential. Defaults to True.</p> <code>True</code> <code>exclude_managed_identity_credential</code> <code>Optional</code> <p>Whether to exclude managed identity from the credential. Defaults to True</p> <code>True</code> <code>exclude_powershell_credential</code> <code>Optional</code> <p>Whether to exclude Azure PowerShell. Defaults to False.</p> <code>False</code> <code>exclude_visual_studio_code_credential</code> <code>Optional</code> <p>Whether to exclude stored credential from VS Code. Defaults to False</p> <code>False</code> <code>exclude_shared_token_cache_credential</code> <code>Optional</code> <p>Whether to exclude the shared token cache. Defaults to False.</p> <code>False</code> <code>exclude_interactive_browser_credential</code> <code>Optional</code> <p>Whether to exclude interactive browser authentication (see InteractiveBrowserCredential). Defaults to False</p> <code>False</code> <code>logging_enable</code> <code>Optional</code> <p>Turn on or off logging. Defaults to False.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/authentication/authenticate.py</code> <pre><code>class DefaultAuth:\n\"\"\"\n    A default credential capable of handling most Azure SDK authentication scenarios.\n\n    The identity it uses depends on the environment. When an access token is needed, it requests one using these identities in turn, stopping when one provides a token:\n\n    1) A service principal configured by environment variables.\n\n    2) An Azure managed identity.\n\n    3) On Windows only: a user who has signed in with a Microsoft application, such as Visual Studio. If multiple identities are in the cache, then the value of the environment variable AZURE_USERNAME is used to select which identity to use.\n\n    4) The user currently signed in to Visual Studio Code.\n\n    5) The identity currently logged in to the Azure CLI.\n\n    6) The identity currently logged in to Azure PowerShell.\n\n    Args:\n        exclude_cli_credential (Optional): Whether to exclude the Azure CLI from the credential. Defaults to False.\n        exclude_environment_credential (Optional): Whether to exclude a service principal configured by environment variables from the credential. Defaults to True.\n        exclude_managed_identity_credential (Optional): Whether to exclude managed identity from the credential. Defaults to True\n        exclude_powershell_credential (Optional): Whether to exclude Azure PowerShell. Defaults to False.\n        exclude_visual_studio_code_credential (Optional): Whether to exclude stored credential from VS Code. Defaults to False\n        exclude_shared_token_cache_credential (Optional): Whether to exclude the shared token cache. Defaults to False.\n        exclude_interactive_browser_credential (Optional): Whether to exclude interactive browser authentication (see InteractiveBrowserCredential). Defaults to False\n        logging_enable (Optional): Turn on or off logging. Defaults to False.\n    \"\"\"\n    def __init__(self, exclude_cli_credential=False, exclude_environment_credential=True, exclude_managed_identity_credential=True, exclude_powershell_credential=False, exclude_visual_studio_code_credential=False, exclude_shared_token_cache_credential=False, exclude_interactive_browser_credential=False, logging_enable=False) -&gt; None:\n        self.exclude_cli_credential=exclude_cli_credential\n        self.exclude_environment_credential=exclude_environment_credential\n        self.exclude_managed_identity_credential=exclude_managed_identity_credential\n        self.exclude_powershell_credential=exclude_powershell_credential\n        self.exclude_visual_studio_code_credential=exclude_visual_studio_code_credential\n        self.exclude_shared_token_cache_credential=exclude_shared_token_cache_credential\n        self.exclude_interactive_browser_credential=exclude_interactive_browser_credential\n        self.logging_enable=logging_enable\n\n    def authenticate(self) -&gt; DefaultAzureCredential:\n\"\"\"\n        A default credential capable of handling most Azure SDK authentication scenarios.\n\n        Returns:\n            DefaultAzureCredential: A default credential capable of handling most Azure SDK authentication scenarios.\n        \"\"\"\n        try:\n            access_token = DefaultAzureCredential(\n                exclude_cli_credential=self.exclude_cli_credential, \n                exclude_environment_credential=self.exclude_environment_credential, \n                exclude_managed_identity_credential=self.exclude_managed_identity_credential, \n                exclude_powershell_credential=self.exclude_powershell_credential, \n                exclude_visual_studio_code_credential=self.exclude_visual_studio_code_credential, \n                exclude_shared_token_cache_credential=self.exclude_shared_token_cache_credential, \n                exclude_interactive_browser_credential=self.exclude_interactive_browser_credential, \n                logging_enable=self.logging_enable)\n            return access_token\n        except Exception as e:\n            logging.exception('error returning default azure credential')\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.authenticate.DefaultAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>A default credential capable of handling most Azure SDK authentication scenarios.</p> <p>Returns:</p> Name Type Description <code>DefaultAzureCredential</code> <code>DefaultAzureCredential</code> <p>A default credential capable of handling most Azure SDK authentication scenarios.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/authenticate.py</code> <pre><code>def authenticate(self) -&gt; DefaultAzureCredential:\n\"\"\"\n    A default credential capable of handling most Azure SDK authentication scenarios.\n\n    Returns:\n        DefaultAzureCredential: A default credential capable of handling most Azure SDK authentication scenarios.\n    \"\"\"\n    try:\n        access_token = DefaultAzureCredential(\n            exclude_cli_credential=self.exclude_cli_credential, \n            exclude_environment_credential=self.exclude_environment_credential, \n            exclude_managed_identity_credential=self.exclude_managed_identity_credential, \n            exclude_powershell_credential=self.exclude_powershell_credential, \n            exclude_visual_studio_code_credential=self.exclude_visual_studio_code_credential, \n            exclude_shared_token_cache_credential=self.exclude_shared_token_cache_credential, \n            exclude_interactive_browser_credential=self.exclude_interactive_browser_credential, \n            logging_enable=self.logging_enable)\n        return access_token\n    except Exception as e:\n        logging.exception('error returning default azure credential')\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/","title":"Json","text":""},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJson","title":"<code>PipelineJobFromJson</code>","text":"<p>         Bases: <code>ConverterInterface</code></p> <p>Converts a json string into a Pipeline Job</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_json</code> <code>str</code> <p>Json representing PipelineJob information, including tasks and related steps</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>class PipelineJobFromJson(ConverterInterface):\n'''\n    Converts a json string into a Pipeline Job\n\n    Args:\n        pipeline_json (str): Json representing PipelineJob information, including tasks and related steps \n    '''\n    pipeline_json: str\n\n    def __init__(self, pipeline_json: str):\n        self.pipeline_json = pipeline_json\n\n    def _try_convert_to_pipeline_secret(self, value):\n        try:\n            if \"pipeline_secret\" in value:\n                value[\"pipeline_secret\"][\"type\"] = getattr(sys.modules[__name__], value[\"pipeline_secret\"][\"type\"])\n            return PipelineSecret.parse_obj(value[\"pipeline_secret\"])\n        except: # NOSONAR\n            return value\n\n    def convert(self) -&gt; PipelineJob:\n'''\n        Converts a json string to a Pipeline Job\n        '''\n        pipeline_job_dict = json.loads(self.pipeline_json)\n\n        # convert string component to class\n        for task in pipeline_job_dict[\"task_list\"]:\n            for step in task[\"step_list\"]:\n                step[\"component\"] = getattr(sys.modules[__name__], step[\"component\"])\n                for param_key, param_value in step[\"component_parameters\"].items():\n                    step[\"component_parameters\"][param_key] = self._try_convert_to_pipeline_secret(param_value)\n                    if not isinstance(step[\"component_parameters\"][param_key], PipelineSecret) and isinstance(param_value, dict):\n                        for key, value in param_value.items():\n                            step[\"component_parameters\"][param_key][key] = self._try_convert_to_pipeline_secret(value) \n\n        return PipelineJob(**pipeline_job_dict)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJson.convert","title":"<code>convert()</code>","text":"<p>Converts a json string to a Pipeline Job</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>def convert(self) -&gt; PipelineJob:\n'''\n    Converts a json string to a Pipeline Job\n    '''\n    pipeline_job_dict = json.loads(self.pipeline_json)\n\n    # convert string component to class\n    for task in pipeline_job_dict[\"task_list\"]:\n        for step in task[\"step_list\"]:\n            step[\"component\"] = getattr(sys.modules[__name__], step[\"component\"])\n            for param_key, param_value in step[\"component_parameters\"].items():\n                step[\"component_parameters\"][param_key] = self._try_convert_to_pipeline_secret(param_value)\n                if not isinstance(step[\"component_parameters\"][param_key], PipelineSecret) and isinstance(param_value, dict):\n                    for key, value in param_value.items():\n                        step[\"component_parameters\"][param_key][key] = self._try_convert_to_pipeline_secret(value) \n\n    return PipelineJob(**pipeline_job_dict)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJson","title":"<code>PipelineJobToJson</code>","text":"<p>         Bases: <code>ConverterInterface</code></p> <p>Converts a Pipeline Job into a json string</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_job</code> <code>PipelineJob</code> <p>A Pipeline Job consisting of tasks and steps</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>class PipelineJobToJson(ConverterInterface):\n'''\n    Converts a Pipeline Job into a json string\n\n    Args:\n        pipeline_job (PipelineJob): A Pipeline Job consisting of tasks and steps\n    '''\n    pipeline_job: PipelineJob\n\n    def __init__(self, pipeline_job: PipelineJob):\n        self.pipeline_job = pipeline_job\n\n    def convert(self):\n'''\n        Converts a Pipeline Job to a json string\n        '''\n        # required because pydantic does not use encoders in subclasses\n        for task in self.pipeline_job.task_list:\n            step_dict_list = []\n            for step in task.step_list:\n                step_dict_list.append(json.loads(step.json(models_as_dict=False, exclude_none=True)))\n            task.step_list = step_dict_list\n\n        pipeline_job_json = self.pipeline_job.json(exclude_none=True)\n        return pipeline_job_json\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJson.convert","title":"<code>convert()</code>","text":"<p>Converts a Pipeline Job to a json string</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>def convert(self):\n'''\n    Converts a Pipeline Job to a json string\n    '''\n    # required because pydantic does not use encoders in subclasses\n    for task in self.pipeline_job.task_list:\n        step_dict_list = []\n        for step in task.step_list:\n            step_dict_list.append(json.loads(step.json(models_as_dict=False, exclude_none=True)))\n        task.step_list = step_dict_list\n\n    pipeline_job_json = self.pipeline_job.json(exclude_none=True)\n    return pipeline_job_json\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks_dbx/","title":"Databricks DBX","text":""},{"location":"sdk/code-reference/pipelines/deploy/databricks_dbx/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksDBXDeploy","title":"<code>DatabricksDBXDeploy</code>","text":"<p>         Bases: <code>DeployInterface</code></p> <p>Deploys an RTDIP Pipeline to Databricks Worflows leveraging Databricks DBX.  For more information about Databricks DBX, please click here.</p> <p>Deploying an RTDIP Pipeline to Databricks requires only a few additional pieces of information to ensure the RTDIP Pipeline Job can be run in Databricks. This information includes:</p> <ul> <li>Cluster: This can be defined a the Job or Task level and includes the size of the cluster to be used for the job</li> <li>Task: The cluster to be used to execute the task, as well as any task scheduling information, if required.</li> </ul> <p>All options available in the Databricks Jobs REST API v2.1 can be configured in the Databricks classes that have been defined in <code>rtdip_sdk.pipelines.deploy.models.databricks</code>, enabling full control of the configuration of the Databricks Workflow :</p> <ul> <li><code>DatabricksJob</code></li> <li><code>DatabricksTask</code></li> </ul> <p>RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component and these will be setup on the Worflow automatically during the Databricks Workflow creation.</p> Example <p>This example assumes that a PipelineJob has already been defined by a variable called <code>pipeline_job</code></p> <pre><code>from rtdip_sdk.pipelines.deploy.databricks import DatabricksDBXDeploy\n\n# Setup a job cluster for Databricks\ndatabricks_job_cluster = DatabricksJobCluster(\n    job_cluster_key=\"test_job_cluster\", \n    new_cluster=DatabricksCluster(\n        spark_version = \"11.3.x-scala2.12\",\n        node_type_id = \"Standard_D3_v2\",\n        num_workers = 2\n    )\n)\n\n# Define the cluster to be leveraged for the Pipeline Task\ndatabricks_task = DatabricksTaskForPipelineTask(\n    name=\"test_task\", \n    job_cluster_key=\"test_job_cluster\"\n)\n\n# Create the Databricks Job for the PipelineJob\ndatabricks_job = DatabricksJobForPipelineJob(\n    job_clusters=[databricks_job_cluster],\n    databricks_task_for_pipeline_task_list=[databricks_task]\n)\n\n# Create an instance of `DatabricksDBXDeploy` and pass the relevant arguments to the class\ndatabricks_job = DatabricksDBXDeploy(\n    pipeline_job=pipeline_job, \n    databricks_job_for_pipeline_job=databricks_job, \n    host=\"https://test.databricks.net\", \n    token=\"test_token\"\n)\n\n# Execute the deploy method to create a Workflow in the specified Databricks Environment\ndeploy_result = databricks_job.deploy()\n\n# If the job should be executed immediately, excute the `launch` method\nlaunch_result = databricks_job.launch()        \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_job</code> <code>PipelineJob</code> <p>Pipeline Job containing tasks and steps that are to be deployed</p> required <code>databricks_job_for_pipeline_job</code> <code>DatabricksJobForPipelineJob</code> <p>Contains Databricks specific information required for deploying the RTDIP Pipeline Job to Databricks, such as cluster and workflow scheduling information. This can be any field in the Databricks Jobs REST API v2.1</p> required <code>host</code> <code>str</code> <p>Databricks URL</p> required <code>token</code> <code>str</code> <p>Token for authenticating with Databricks such as a Databricks PAT Token or Azure AD Token</p> required <code>workspace_directory</code> <code>str</code> <p>Determines the folder location in the Databricks Workspace. Defaults to /rtdip </p> <code>'/rtdip'</code> <code>artifacts_directory</code> <code>str</code> <p>Determines the folder location in the Databricks Workspace. Defaults to dbfs:/rtdip/projects</p> <code>'dbfs:/rtdip/projects'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>class DatabricksDBXDeploy(DeployInterface):\n'''\n    Deploys an RTDIP Pipeline to Databricks Worflows leveraging Databricks DBX.  For more information about Databricks DBX, please click [here.](https://dbx.readthedocs.io/en/latest/)\n\n    Deploying an RTDIP Pipeline to Databricks requires only a few additional pieces of information to ensure the RTDIP Pipeline Job can be run in Databricks. This information includes:\n\n    - **Cluster**: This can be defined a the Job or Task level and includes the size of the cluster to be used for the job\n    - **Task**: The cluster to be used to execute the task, as well as any task scheduling information, if required.\n\n    All options available in the [Databricks Jobs REST API v2.1](https://docs.databricks.com/dev-tools/api/latest/jobs.html) can be configured in the Databricks classes that have been defined in `rtdip_sdk.pipelines.deploy.models.databricks`, enabling full control of the configuration of the Databricks Workflow :\n\n    - `DatabricksJob`\n    - `DatabricksTask`\n\n    RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component and these will be setup on the Worflow automatically during the Databricks Workflow creation.\n\n    Example:\n        This example assumes that a PipelineJob has already been defined by a variable called `pipeline_job`\n\n        ```python\n        from rtdip_sdk.pipelines.deploy.databricks import DatabricksDBXDeploy\n\n        # Setup a job cluster for Databricks\n        databricks_job_cluster = DatabricksJobCluster(\n            job_cluster_key=\"test_job_cluster\", \n            new_cluster=DatabricksCluster(\n                spark_version = \"11.3.x-scala2.12\",\n                node_type_id = \"Standard_D3_v2\",\n                num_workers = 2\n            )\n        )\n\n        # Define the cluster to be leveraged for the Pipeline Task\n        databricks_task = DatabricksTaskForPipelineTask(\n            name=\"test_task\", \n            job_cluster_key=\"test_job_cluster\"\n        )\n\n        # Create the Databricks Job for the PipelineJob\n        databricks_job = DatabricksJobForPipelineJob(\n            job_clusters=[databricks_job_cluster],\n            databricks_task_for_pipeline_task_list=[databricks_task]\n        )\n\n        # Create an instance of `DatabricksDBXDeploy` and pass the relevant arguments to the class\n        databricks_job = DatabricksDBXDeploy(\n            pipeline_job=pipeline_job, \n            databricks_job_for_pipeline_job=databricks_job, \n            host=\"https://test.databricks.net\", \n            token=\"test_token\"\n        )\n\n        # Execute the deploy method to create a Workflow in the specified Databricks Environment\n        deploy_result = databricks_job.deploy()\n\n        # If the job should be executed immediately, excute the `launch` method\n        launch_result = databricks_job.launch()        \n        ```\n\n    Args:\n        pipeline_job (PipelineJob): Pipeline Job containing tasks and steps that are to be deployed\n        databricks_job_for_pipeline_job (DatabricksJobForPipelineJob): Contains Databricks specific information required for deploying the RTDIP Pipeline Job to Databricks, such as cluster and workflow scheduling information. This can be any field in the [Databricks Jobs REST API v2.1](https://docs.databricks.com/dev-tools/api/latest/jobs.html)\n        host (str): Databricks URL\n        token (str): Token for authenticating with Databricks such as a Databricks PAT Token or Azure AD Token\n        workspace_directory (str, optional): Determines the folder location in the Databricks Workspace. Defaults to /rtdip \n        artifacts_directory (str, optional): Determines the folder location in the Databricks Workspace. Defaults to dbfs:/rtdip/projects\n    '''\n    pipeline_job: PipelineJob\n    databricks_job_for_pipeline_job: DatabricksJobForPipelineJob\n    host: str\n    token: str\n    workspace_directory: str\n    artifacts_directory: str\n\n    def __init__(self, pipeline_job: PipelineJob, databricks_job_for_pipeline_job: DatabricksJobForPipelineJob, host: str, token: str, workspace_directory: str = \"/rtdip\", artifacts_directory: str = \"dbfs:/rtdip/projects\") -&gt; None:\n        self.pipeline_job = pipeline_job\n        self.databricks_job_for_pipeline_job = databricks_job_for_pipeline_job\n        self.host = host\n        self.token = token\n        self.workspace_directory = workspace_directory\n        self.artifacts_directory = artifacts_directory\n\n\n    def deploy(self) -&gt; bool:\n'''\n        Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Pipeline Job Name and therefore will overwrite any existing workflow in Databricks with the same name.\n\n        DBX packages the pipeline job into a python wheel that is uploaded as an artifact in the dbfs and creates the relevant tasks as specified by the Databricks Jobs REST API. \n        '''\n\n        # Setup folder \n        current_dir = os.getcwd()\n        dbx_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx\"\n        project_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/.dbx\"\n        build_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/build\"\n        dist_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/dist\"\n        egg_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/{}.egg-info\".format(self.pipeline_job.name)\n        if os.path.exists(project_path):\n            shutil.rmtree(project_path, ignore_errors=True)\n        if os.path.exists(build_path):\n            shutil.rmtree(build_path, ignore_errors=True)\n        if os.path.exists(dist_path):\n            shutil.rmtree(dist_path, ignore_errors=True)\n        if os.path.exists(egg_path):\n            shutil.rmtree(egg_path, ignore_errors=True)\n\n        os.chdir(dbx_path)\n\n        # create Databricks Job Tasks\n        databricks_tasks = []\n        for task in self.pipeline_job.task_list:\n            databricks_job_task = DatabricksTask(task_key=task.name, libraries=[], depends_on=[])\n            if self.databricks_job_for_pipeline_job.databricks_task_for_pipeline_task_list is not None:\n                databricks_task_for_pipeline_task = next(x for x in self.databricks_job_for_pipeline_job.databricks_task_for_pipeline_task_list if x.name == task.name)\n                if databricks_task_for_pipeline_task is not None:\n                    databricks_job_task.__dict__.update(databricks_task_for_pipeline_task.__dict__)\n\n            databricks_job_task.name = task.name\n            databricks_job_task.depends_on = task.depends_on_task\n\n            # get libraries\n            for step in task.step_list:\n                libraries = step.component.libraries()\n                for pypi_library in libraries.pypi_libraries:\n                    databricks_job_task.libraries.append(DatabricksLibraries(pypi=DatbricksLibrariesPypi(package=pypi_library.to_string(), repo=pypi_library.repo)))\n                for maven_library in libraries.maven_libraries:\n                    databricks_job_task.libraries.append(DatabricksLibraries(maven=DatabricksLibrariesMaven(coordinates=maven_library.to_string(), repo=maven_library.repo)))\n                for wheel_library in libraries.pythonwheel_libraries:\n                    databricks_job_task.libraries.append(DatabricksLibraries(whl=wheel_library))\n\n            try:\n                rtdip_version = version(\"rtdip-sdk\")\n                databricks_job_task.libraries.append(DatabricksLibraries(pypi=DatbricksLibrariesPypi(package=\"rtdip-sdk[pipelines]=={}\".format(rtdip_version))))\n            except PackageNotFoundError as e:\n                databricks_job_task.libraries.append(DatabricksLibraries(pypi=DatbricksLibrariesPypi(package=\"rtdip-sdk[pipelines]\")))\n\n            databricks_job_task.spark_python_task = DatabricksSparkPythonTask(\n                python_file=\"file://{}\".format(\"rtdip/tasks/pipeline_task.py\"),\n                parameters=[PipelineJobToJson(self.pipeline_job).convert()]\n            )\n            databricks_tasks.append(databricks_job_task)\n\n        databricks_job = DatabricksJob(name=self.pipeline_job.name, tasks=databricks_tasks)\n        databricks_job.__dict__.update(self.databricks_job_for_pipeline_job.__dict__)\n        databricks_job.__dict__.pop(\"databricks_task_for_pipeline_task_list\", None)\n\n        # Setup Project \n        environment = {\n            \"profile\": \"rtdip\",\n            \"storage_type\": \"mlflow\",\n            \"properties\": {\n                \"workspace_directory\": \"{}/{}/\".format(self.workspace_directory, self.pipeline_job.name.lower()),\n                \"artifact_location\": \"{}/{}\".format(self.artifacts_directory, self.pipeline_job.name.lower())\n            }            \n        }\n        project = DatabricksDBXProject(\n            environments={\"rtdip\": environment},\n            inplace_jinja_support=True,\n            failsafe_cluster_reuse_with_assets=False,\n            context_based_upload_for_execute=False\n        )\n\n        # create project file\n        if not os.path.exists(project_path):\n            os.mkdir(project_path)\n\n        with open(project_path + \"/project.json\", \"w\") as f:\n            json.dump(project.dict(), f)\n\n        # create Databricks DBX Environment\n        os.environ[ProfileEnvConfigProvider.DBX_PROFILE_ENV] = json.dumps(environment)\n\n        os.environ[\"RTDIP_DEPLOYMENT_CONFIGURATION\"] = json.dumps({\n            \"environments\": { \n                \"rtdip\": {\"workflows\": [databricks_job.dict(exclude_none=True)]}\n            }\n        })\n\n        # set authentication environment variables\n        os.environ[\"DATABRICKS_HOST\"] = self.host\n        os.environ[\"DATABRICKS_TOKEN\"] = self.token\n\n        os.environ[\"RTDIP_PACKAGE_NAME\"] = self.pipeline_job.name.lower()\n        os.environ[\"RTDIP_PACKAGE_DESCRIPTION\"] = self.pipeline_job.description\n        os.environ[\"RTDIP_PACKAGE_VERSION\"] = self.pipeline_job.version\n\n        # Create Databricks DBX Job\n        dbx_deploy(\n            workflow_name=self.pipeline_job.name,\n            workflow_names=None,\n            job_names=None,\n            deployment_file=Path(\"conf/deployment.json.j2\"),\n            environment_name=\"rtdip\",\n            requirements_file=None,\n            jinja_variables_file=None,\n            branch_name=None,\n            tags=[],\n            headers=[],\n            no_rebuild=False,\n            no_package=False,\n            files_only=False,\n            assets_only=False,\n            write_specs_to_file=None,\n            debug=False,\n        )\n        if os.path.exists(build_path):\n            shutil.rmtree(build_path, ignore_errors=True)\n        if os.path.exists(dist_path):\n            shutil.rmtree(dist_path, ignore_errors=True)\n        if os.path.exists(egg_path):\n            shutil.rmtree(egg_path, ignore_errors=True)\n        os.chdir(current_dir)\n\n        return True\n\n    def launch(self):\n'''\n        Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Run Now` in Databricks Workflows\n        '''        \n        # set authentication environment variables\n        os.environ[\"DATABRICKS_HOST\"] = self.host\n        os.environ[\"DATABRICKS_TOKEN\"] = self.token\n\n        #launch job        \n        current_dir = os.getcwd()\n        dbx_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx\"\n        os.chdir(dbx_path)\n        dbx_launch(\n            workflow_name=self.pipeline_job.name,\n            environment_name=\"rtdip\",\n            job_name=None,\n            is_pipeline=False,\n            trace=False,\n            kill_on_sigterm=False,\n            existing_runs=\"pass\",\n            as_run_submit=False,\n            from_assets=False,\n            tags=[],\n            branch_name=None,\n            include_output=None,\n            headers=None,\n            parameters=None,\n            debug=None\n        )\n        os.chdir(current_dir)\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks_dbx/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksDBXDeploy.deploy","title":"<code>deploy()</code>","text":"<p>Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Pipeline Job Name and therefore will overwrite any existing workflow in Databricks with the same name.</p> <p>DBX packages the pipeline job into a python wheel that is uploaded as an artifact in the dbfs and creates the relevant tasks as specified by the Databricks Jobs REST API.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def deploy(self) -&gt; bool:\n'''\n    Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Pipeline Job Name and therefore will overwrite any existing workflow in Databricks with the same name.\n\n    DBX packages the pipeline job into a python wheel that is uploaded as an artifact in the dbfs and creates the relevant tasks as specified by the Databricks Jobs REST API. \n    '''\n\n    # Setup folder \n    current_dir = os.getcwd()\n    dbx_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx\"\n    project_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/.dbx\"\n    build_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/build\"\n    dist_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/dist\"\n    egg_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx/{}.egg-info\".format(self.pipeline_job.name)\n    if os.path.exists(project_path):\n        shutil.rmtree(project_path, ignore_errors=True)\n    if os.path.exists(build_path):\n        shutil.rmtree(build_path, ignore_errors=True)\n    if os.path.exists(dist_path):\n        shutil.rmtree(dist_path, ignore_errors=True)\n    if os.path.exists(egg_path):\n        shutil.rmtree(egg_path, ignore_errors=True)\n\n    os.chdir(dbx_path)\n\n    # create Databricks Job Tasks\n    databricks_tasks = []\n    for task in self.pipeline_job.task_list:\n        databricks_job_task = DatabricksTask(task_key=task.name, libraries=[], depends_on=[])\n        if self.databricks_job_for_pipeline_job.databricks_task_for_pipeline_task_list is not None:\n            databricks_task_for_pipeline_task = next(x for x in self.databricks_job_for_pipeline_job.databricks_task_for_pipeline_task_list if x.name == task.name)\n            if databricks_task_for_pipeline_task is not None:\n                databricks_job_task.__dict__.update(databricks_task_for_pipeline_task.__dict__)\n\n        databricks_job_task.name = task.name\n        databricks_job_task.depends_on = task.depends_on_task\n\n        # get libraries\n        for step in task.step_list:\n            libraries = step.component.libraries()\n            for pypi_library in libraries.pypi_libraries:\n                databricks_job_task.libraries.append(DatabricksLibraries(pypi=DatbricksLibrariesPypi(package=pypi_library.to_string(), repo=pypi_library.repo)))\n            for maven_library in libraries.maven_libraries:\n                databricks_job_task.libraries.append(DatabricksLibraries(maven=DatabricksLibrariesMaven(coordinates=maven_library.to_string(), repo=maven_library.repo)))\n            for wheel_library in libraries.pythonwheel_libraries:\n                databricks_job_task.libraries.append(DatabricksLibraries(whl=wheel_library))\n\n        try:\n            rtdip_version = version(\"rtdip-sdk\")\n            databricks_job_task.libraries.append(DatabricksLibraries(pypi=DatbricksLibrariesPypi(package=\"rtdip-sdk[pipelines]=={}\".format(rtdip_version))))\n        except PackageNotFoundError as e:\n            databricks_job_task.libraries.append(DatabricksLibraries(pypi=DatbricksLibrariesPypi(package=\"rtdip-sdk[pipelines]\")))\n\n        databricks_job_task.spark_python_task = DatabricksSparkPythonTask(\n            python_file=\"file://{}\".format(\"rtdip/tasks/pipeline_task.py\"),\n            parameters=[PipelineJobToJson(self.pipeline_job).convert()]\n        )\n        databricks_tasks.append(databricks_job_task)\n\n    databricks_job = DatabricksJob(name=self.pipeline_job.name, tasks=databricks_tasks)\n    databricks_job.__dict__.update(self.databricks_job_for_pipeline_job.__dict__)\n    databricks_job.__dict__.pop(\"databricks_task_for_pipeline_task_list\", None)\n\n    # Setup Project \n    environment = {\n        \"profile\": \"rtdip\",\n        \"storage_type\": \"mlflow\",\n        \"properties\": {\n            \"workspace_directory\": \"{}/{}/\".format(self.workspace_directory, self.pipeline_job.name.lower()),\n            \"artifact_location\": \"{}/{}\".format(self.artifacts_directory, self.pipeline_job.name.lower())\n        }            \n    }\n    project = DatabricksDBXProject(\n        environments={\"rtdip\": environment},\n        inplace_jinja_support=True,\n        failsafe_cluster_reuse_with_assets=False,\n        context_based_upload_for_execute=False\n    )\n\n    # create project file\n    if not os.path.exists(project_path):\n        os.mkdir(project_path)\n\n    with open(project_path + \"/project.json\", \"w\") as f:\n        json.dump(project.dict(), f)\n\n    # create Databricks DBX Environment\n    os.environ[ProfileEnvConfigProvider.DBX_PROFILE_ENV] = json.dumps(environment)\n\n    os.environ[\"RTDIP_DEPLOYMENT_CONFIGURATION\"] = json.dumps({\n        \"environments\": { \n            \"rtdip\": {\"workflows\": [databricks_job.dict(exclude_none=True)]}\n        }\n    })\n\n    # set authentication environment variables\n    os.environ[\"DATABRICKS_HOST\"] = self.host\n    os.environ[\"DATABRICKS_TOKEN\"] = self.token\n\n    os.environ[\"RTDIP_PACKAGE_NAME\"] = self.pipeline_job.name.lower()\n    os.environ[\"RTDIP_PACKAGE_DESCRIPTION\"] = self.pipeline_job.description\n    os.environ[\"RTDIP_PACKAGE_VERSION\"] = self.pipeline_job.version\n\n    # Create Databricks DBX Job\n    dbx_deploy(\n        workflow_name=self.pipeline_job.name,\n        workflow_names=None,\n        job_names=None,\n        deployment_file=Path(\"conf/deployment.json.j2\"),\n        environment_name=\"rtdip\",\n        requirements_file=None,\n        jinja_variables_file=None,\n        branch_name=None,\n        tags=[],\n        headers=[],\n        no_rebuild=False,\n        no_package=False,\n        files_only=False,\n        assets_only=False,\n        write_specs_to_file=None,\n        debug=False,\n    )\n    if os.path.exists(build_path):\n        shutil.rmtree(build_path, ignore_errors=True)\n    if os.path.exists(dist_path):\n        shutil.rmtree(dist_path, ignore_errors=True)\n    if os.path.exists(egg_path):\n        shutil.rmtree(egg_path, ignore_errors=True)\n    os.chdir(current_dir)\n\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks_dbx/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksDBXDeploy.launch","title":"<code>launch()</code>","text":"<p>Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a <code>Run Now</code> in Databricks Workflows</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def launch(self):\n'''\n    Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Run Now` in Databricks Workflows\n    '''        \n    # set authentication environment variables\n    os.environ[\"DATABRICKS_HOST\"] = self.host\n    os.environ[\"DATABRICKS_TOKEN\"] = self.token\n\n    #launch job        \n    current_dir = os.getcwd()\n    dbx_path = os.path.dirname(os.path.abspath(__file__)) + \"/dbx\"\n    os.chdir(dbx_path)\n    dbx_launch(\n        workflow_name=self.pipeline_job.name,\n        environment_name=\"rtdip\",\n        job_name=None,\n        is_pipeline=False,\n        trace=False,\n        kill_on_sigterm=False,\n        existing_runs=\"pass\",\n        as_run_submit=False,\n        from_assets=False,\n        tags=[],\n        branch_name=None,\n        include_output=None,\n        headers=None,\n        parameters=None,\n        debug=None\n    )\n    os.chdir(current_dir)\n\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/","title":"Write to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination","title":"<code>SparkDeltaDestination</code>","text":"<p>         Bases: <code>DestinationInterface</code></p> <p>The Spark Delta Source is used to write data to a Delta table. </p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the Hive Metastore or Unity Catalog Delta Table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>mode</code> <code>str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)</p> <code>'append'</code> <code>trigger</code> <code>str</code> <p>Frequency of the write operation</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'DeltaDestination'</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>txnAppId</code> <code>str</code> <p>A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)</p> <code>txnVersion</code> <code>str</code> <p>A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)</p> <code>maxRecordsPerFile</code> <code>int str</code> <p>Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)</p> <code>replaceWhere</code> <code>str</code> <p>Condition(s) for overwriting. (Batch)</p> <code>partitionOverwriteMode</code> <code>str</code> <p>When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)</p> <code>overwriteSchema</code> <code>bool str</code> <p>If True, overwrites the schema as well as the table data. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>class SparkDeltaDestination(DestinationInterface):\n'''\n    The Spark Delta Source is used to write data to a Delta table. \n\n    Args:\n        table_name (str): Name of the Hive Metastore or Unity Catalog Delta Table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        mode (str): Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)\n        trigger (str): Frequency of the write operation\n        query_name (str): Unique name for the query in associated SparkSession\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        txnAppId (str): A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)\n        txnVersion (str): A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)\n        maxRecordsPerFile (int str): Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)\n        replaceWhere (str): Condition(s) for overwriting. (Batch)\n        partitionOverwriteMode (str): When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)\n        overwriteSchema (bool str): If True, overwrites the schema as well as the table data. (Batch)\n    '''\n    table_name: str\n    options: dict\n    mode: str\n    trigger: str\n    query_name: str\n\n    def __init__(self, table_name:str, options: dict, mode: str = \"append\", trigger=\"10 seconds\", query_name=\"DeltaDestination\") -&gt; None:\n        self.table_name = table_name\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        '''             \n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(DEFAULT_PACKAGES[\"spark_delta_core\"])\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {\n            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n        }\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self, df: DataFrame):\n'''\n        Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n        '''\n        try:\n            return (\n                df\n                .write\n                .format(\"delta\")\n                .mode(self.mode)\n                .options(**self.options)\n                .saveAsTable(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self, df: DataFrame) -&gt; DataFrame:\n'''\n        Writes streaming data to Delta. Exactly-once processing is guaranteed\n        '''\n        try:\n            query = (df\n                .writeStream\n                .trigger(processingTime=self.trigger)\n                .format(\"delta\")\n                .queryName(self.query_name)\n                .outputMode(self.mode)\n                .options(**self.options)\n                .toTable(self.table_name)\n            )\n\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(10)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    '''             \n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.write_batch","title":"<code>write_batch(df)</code>","text":"<p>Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_batch(self, df: DataFrame):\n'''\n    Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n    '''\n    try:\n        return (\n            df\n            .write\n            .format(\"delta\")\n            .mode(self.mode)\n            .options(**self.options)\n            .saveAsTable(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.write_stream","title":"<code>write_stream(df)</code>","text":"<p>Writes streaming data to Delta. Exactly-once processing is guaranteed</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_stream(self, df: DataFrame) -&gt; DataFrame:\n'''\n    Writes streaming data to Delta. Exactly-once processing is guaranteed\n    '''\n    try:\n        query = (df\n            .writeStream\n            .trigger(processingTime=self.trigger)\n            .format(\"delta\")\n            .queryName(self.query_name)\n            .outputMode(self.mode)\n            .options(**self.options)\n            .toTable(self.table_name)\n        )\n\n        while query.isActive:\n            if query.lastProgress:\n                logging.info(query.lastProgress)\n            time.sleep(10)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/","title":"Write to Eventhub","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination","title":"<code>SparkEventhubDestination</code>","text":"<p>         Bases: <code>DestinationInterface</code></p> <p>This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configuration which can be found here. If using startingPosition or endingPosition make sure to check out Event Position section for more details and examples.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found here.</p> required <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>class SparkEventhubDestination(DestinationInterface):\n'''\n    This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configuration which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out **Event Position** section for more details and examples.\n\n    Args:\n        options (dict): A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n    '''\n    options: dict\n\n    def __init__(self, options: dict) -&gt; None:\n        self.options = options\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        '''             \n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(DEFAULT_PACKAGES[\"spark_azure_eventhub\"])\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self, df: DataFrame):\n'''\n        Writes batch data to Eventhubs.\n        '''\n        try:\n            return (\n                df\n                .write\n                .format(\"eventhubs\")\n                .options(**self.options)\n                .save()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self, df: DataFrame):\n'''\n        Writes steaming data to Eventhubs.\n        '''\n        try:\n            query = (df\n                .writeStream\n                .format(\"eventhubs\")\n                .options(**self.options)\n                .start()\n            )\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(10)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    '''             \n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.write_batch","title":"<code>write_batch(df)</code>","text":"<p>Writes batch data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_batch(self, df: DataFrame):\n'''\n    Writes batch data to Eventhubs.\n    '''\n    try:\n        return (\n            df\n            .write\n            .format(\"eventhubs\")\n            .options(**self.options)\n            .save()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.write_stream","title":"<code>write_stream(df)</code>","text":"<p>Writes steaming data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_stream(self, df: DataFrame):\n'''\n    Writes steaming data to Eventhubs.\n    '''\n    try:\n        query = (df\n            .writeStream\n            .format(\"eventhubs\")\n            .options(**self.options)\n            .start()\n        )\n        while query.isActive:\n            if query.lastProgress:\n                logging.info(query.lastProgress)\n            time.sleep(10)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/","title":"Databricks Secret Scope","text":""},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets","title":"<code>DatabricksSecrets</code>","text":"<p>         Bases: <code>SecretsInterface</code></p> <p>Reads secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see here.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>vault</code> <code>str</code> <p>Name of the Databricks Secret Scope</p> required <code>key</code> <code>str</code> <p>Name/Key of the secret in the Databricks Secret Scope</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>class DatabricksSecrets(SecretsInterface):\n'''\n    Reads secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see [here.](https://docs.databricks.com/security/secrets/secret-scopes.html)\n\n    Args:\n        spark: Spark Session required to read data from a Delta table\n        vault: Name of the Databricks Secret Scope\n        key: Name/Key of the secret in the Databricks Secret Scope\n    '''    \n    spark: SparkSession\n    vault: str\n    key: str\n\n    def __init__(self, spark: SparkSession, vault: str, key: str):\n        self.spark = spark\n        self.vault = vault\n        self.key = key\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        '''        \n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def get(self):\n'''\n        Retrieves the secret from the Databricks Secret Scope\n        '''        \n        dbutils = get_dbutils(self.spark)\n        return dbutils.secrets.get(scope=self.vault, key=self.key)\n\n    def set(self):\n'''\n        Sets the secret in the Secret Scope\n        Raises:\n            NotImplementedError: Will be implemented at a later point in time\n        '''          \n        return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Databricks Secret Scope</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def get(self):\n'''\n    Retrieves the secret from the Databricks Secret Scope\n    '''        \n    dbutils = get_dbutils(self.spark)\n    return dbutils.secrets.get(scope=self.vault, key=self.key)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.set","title":"<code>set()</code>","text":"<p>Sets the secret in the Secret Scope</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Will be implemented at a later point in time</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def set(self):\n'''\n    Sets the secret in the Secret Scope\n    Raises:\n        NotImplementedError: Will be implemented at a later point in time\n    '''          \n    return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    '''        \n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/","title":"Read from Autoloader","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource","title":"<code>DataBricksAutoLoaderSource</code>","text":"<p>         Bases: <code>SourceInterface</code></p> <p>The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available here</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for configuring the Auto Loader. Further information on the options available are here</p> required <code>path</code> <code>str</code> <p>The cloud storage path</p> required <code>format</code> <code>str</code> <p>Specifies the file format to be read. Supported formats are available here</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>class DataBricksAutoLoaderSource(SourceInterface):\n'''\n    The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available [here](https://docs.databricks.com/ingestion/auto-loader/index.html)\n\n    Args:\n        spark: Spark Session required to read data from cloud storage\n        options: Options that can be specified for configuring the Auto Loader. Further information on the options available are [here](https://docs.databricks.com/ingestion/auto-loader/options.html)\n        path: The cloud storage path\n        format: Specifies the file format to be read. Supported formats are available [here](https://docs.databricks.com/ingestion/auto-loader/options.html#file-format-options)\n    ''' \n    spark: SparkSession\n    options: dict\n    path: str\n\n    def __init__(self, spark: SparkSession, options: dict, path: str, format: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.path = path\n        self.options[\"cloudFiles.format\"] = format\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        '''        \n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(DEFAULT_PACKAGES[\"spark_delta_core\"])\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self, df: DataFrame):\n        return True\n\n    def read_batch(self):\n'''\n        Raises:\n            NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n        '''\n        raise NotImplementedError(\"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\")\n\n    def read_stream(self) -&gt; DataFrame:\n'''\n        Performs streaming reads of files in cloud storage.\n        '''\n        try:\n            return (self.spark\n                .readStream\n                .format(\"cloudFiles\")\n                .options(**self.options)\n                .load(self.path)\n            )\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be <code>availableNow=True</code> to perform batch-like reads of cloud storage files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_batch(self):\n'''\n    Raises:\n        NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n    '''\n    raise NotImplementedError(\"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Performs streaming reads of files in cloud storage.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n'''\n    Performs streaming reads of files in cloud storage.\n    '''\n    try:\n        return (self.spark\n            .readStream\n            .format(\"cloudFiles\")\n            .options(**self.options)\n            .load(self.path)\n        )\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    '''        \n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/","title":"Read from Delta","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource","title":"<code>SparkDeltaSource</code>","text":"<p>         Bases: <code>SourceInterface</code></p> <p>The Spark Delta Source is used to read data from a Delta table. </p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>table_name</code> <code>str</code> <p>Name of the Hive Metastore or Unity Catalog Delta Table</p> required <p>Attributes:</p> Name Type Description <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>withEventTimeOrder</code> <code>bool str</code> <p>Whether the initial snapshot should be processed with event time order. (Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>class SparkDeltaSource(SourceInterface):\n'''\n    The Spark Delta Source is used to read data from a Delta table. \n\n    Args:\n        spark: Spark Session required to read data from a Delta table\n        options: Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#read-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-source){ target=\"_blank\" }.\n        table_name: Name of the Hive Metastore or Unity Catalog Delta Table\n\n    Attributes:\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        withEventTimeOrder (bool str): Whether the initial snapshot should be processed with event time order. (Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    ''' \n    spark: SparkSession\n    options: dict\n    table_name: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_name: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_name = table_name\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        '''            \n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(DEFAULT_PACKAGES[\"spark_delta_core\"])\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n'''\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        '''\n        try:\n            return (self.spark\n                .read\n                .format(\"delta\")\n                .options(**self.options)\n                .table(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n'''\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        '''\n        try:\n            return (self.spark\n                .readStream\n                .format(\"delta\")\n                .options(**self.options)\n                .load(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_batch(self):\n'''\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    '''\n    try:\n        return (self.spark\n            .read\n            .format(\"delta\")\n            .options(**self.options)\n            .table(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n'''\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    '''\n    try:\n        return (self.spark\n            .readStream\n            .format(\"delta\")\n            .options(**self.options)\n            .load(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    '''            \n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/","title":"Read from Delta sharing","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource","title":"<code>SparkDeltaSharingSource</code>","text":"<p>         Bases: <code>SourceInterface</code></p> <p>The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured </p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available here</p> required <code>table_path</code> <code>str</code> <p>Path to credentials file and Delta table to query</p> required <p>Attributes:</p> Name Type Description <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>readChangeFeed</code> <code>bool str</code> <p>Stream read the change data feed of the shared table. (Batch &amp; Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>class SparkDeltaSharingSource(SourceInterface):\n'''\n    The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured \n\n    Args:\n        spark: Spark Session required to read data from a Delta table\n        options: Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available [here](https://docs.databricks.com/data-sharing/read-data-open.html#apache-spark-read-shared-data){ target=\"_blank\" }\n        table_path: Path to credentials file and Delta table to query\n\n    Attributes:\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        readChangeFeed (bool str): Stream read the change data feed of the shared table. (Batch &amp; Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    ''' \n\n    spark: SparkSession\n    options: dict\n    table_path: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_path: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_path = table_path\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        '''          \n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(DEFAULT_PACKAGES[\"spark_delta_sharing\"])\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n'''\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        '''\n        try:\n            return (self.spark\n                .read\n                .format(\"deltaSharing\")\n                .options(**self.options)\n                .table(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n'''\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        '''\n        try:\n            return (self.spark\n                .readStream\n                .format(\"deltaSharing\")\n                .options(**self.options)\n                .load(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_batch(self):\n'''\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    '''\n    try:\n        return (self.spark\n            .read\n            .format(\"deltaSharing\")\n            .options(**self.options)\n            .table(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n'''\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    '''\n    try:\n        return (self.spark\n            .readStream\n            .format(\"deltaSharing\")\n            .options(**self.options)\n            .load(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    '''          \n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/","title":"Read from an Eventhub","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource","title":"<code>SparkEventhubSource</code>","text":"<p>         Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configuration which can be found here. If using startingPosition or endingPosition make sure to check out Event Position section for more details and examples.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>class SparkEventhubSource(SourceInterface):\n'''\n    This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configuration which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out **Event Position** section for more details and examples.\n    Args:\n        spark: Spark Session\n        options: A dictionary of Eventhub configurations (See Attributes table below)\n\n    Attributes:\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n\n    '''\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = EVENTHUB_SCHEMA\n\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        '''            \n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(DEFAULT_PACKAGES[\"spark_azure_eventhub\"])\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n'''\n        Reads batch data from Eventhubs.\n        '''\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(self.options[eventhub_connection_string])\n\n            return (self.spark\n                .read\n                .format(\"eventhubs\")\n                .options(**self.options)\n                .load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n'''\n        Reads streaming data from Eventhubs.\n        '''\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(self.options[eventhub_connection_string])\n\n            return (self.spark\n                .readStream\n                .format(\"eventhubs\")\n                .options(**self.options)\n                .load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n'''\n    Reads batch data from Eventhubs.\n    '''\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(self.options[eventhub_connection_string])\n\n        return (self.spark\n            .read\n            .format(\"eventhubs\")\n            .options(**self.options)\n            .load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n'''\n    Reads streaming data from Eventhubs.\n    '''\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(self.options[eventhub_connection_string])\n\n        return (self.spark\n            .readStream\n            .format(\"eventhubs\")\n            .options(**self.options)\n            .load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    '''            \n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/eventhub/","title":"Read from an Eventhub","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.eventhub.EventhubBodyBinaryToString","title":"<code>EventhubBodyBinaryToString</code>","text":"<p>         Bases: <code>TransformerInterface</code></p> <p>Converts the Eventhub dataframe body column from a binary to a string.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/eventhub.py</code> <pre><code>class EventhubBodyBinaryToString(TransformerInterface):\n'''\n    Converts the Eventhub dataframe body column from a binary to a string.\n    ''' \n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        '''\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self, df: DataFrame) -&gt; DataFrame:\n'''\n        Args:\n            df (DataFrame): A dataframe based on the structure of the Spark Eventhub connector.\n\n        Returns:\n            DataFrame: A dataframe with the body column converted to string.\n        '''\n        return (\n            df\n            .withColumn(\"body\", df[\"body\"].cast(\"string\"))\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.eventhub.EventhubBodyBinaryToString.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    '''\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.eventhub.EventhubBodyBinaryToString.transform","title":"<code>transform(df)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe based on the structure of the Spark Eventhub connector.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the body column converted to string.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/eventhub.py</code> <pre><code>def transform(self, df: DataFrame) -&gt; DataFrame:\n'''\n    Args:\n        df (DataFrame): A dataframe based on the structure of the Spark Eventhub connector.\n\n    Returns:\n        DataFrame: A dataframe with the body column converted to string.\n    '''\n    return (\n        df\n        .withColumn(\"body\", df[\"body\"].cast(\"string\"))\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/","title":"Delta Table Create","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility","title":"<code>DeltaTableCreateUtility</code>","text":"<p>         Bases: <code>UtilitiesInterface</code></p> <p>Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>columns</code> <code>list[StructField]</code> <p>List of columns and their related column properties</p> required <code>partitioned_by</code> <code>list[str]</code> <p>List of column names to partition the table by</p> <code>None</code> <code>location</code> <code>str</code> <p>Path to storage location</p> <code>None</code> <code>properties</code> <code>dict</code> <p>Propoerties that can be specified for a Delta Table. Further information on the options available are here</p> <code>None</code> <code>comment</code> <code>str</code> <p>Provides a comment on the table metadata</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>class DeltaTableCreateUtility(UtilitiesInterface):\n'''\n    Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.\n\n    Args:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        columns (list[StructField]): List of columns and their related column properties\n        partitioned_by (list[str], optional): List of column names to partition the table by\n        location (str, optional): Path to storage location\n        properties (dict, optional): Propoerties that can be specified for a Delta Table. Further information on the options available are [here](https://docs.databricks.com/delta/table-properties.html#delta-table-properties)\n        comment (str, optional): Provides a comment on the table metadata\n    ''' \n    spark: SparkSession\n    table_name: str\n    columns: list[StructField]\n    partitioned_by: list[str]\n    location: str\n    properties: dict\n    comment: str\n\n    def __init__(self, spark: SparkSession, table_name: str, columns: list[StructField], partitioned_by: list[str] = None, location: str = None, properties: dict = None, comment: str = None) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.columns = columns\n        self.partitioned_by = partitioned_by\n        self.location = location\n        self.properties = properties\n        self.comment = comment\n\n    @staticmethod\n    def system_type():\n'''\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        '''            \n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(DEFAULT_PACKAGES[\"spark_delta_core\"])\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            delta_table = (\n                DeltaTable\n                .createIfNotExists(self.spark)\n                .tableName(self.table_name)\n                .addColumns(self.columns)\n            )\n\n            if self.partitioned_by is not None:\n                delta_table = delta_table.partitionedBy(self.partitioned_by)\n\n            if self.location is not None:\n                delta_table = delta_table.location(self.location)\n\n            if self.properties is not None:\n                for key, value in self.properties.items():\n                    delta_table = delta_table.property(key, value)\n\n            if self.comment is not None:\n                delta_table = delta_table.comment(self.comment)\n\n            delta_table.execute()\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>@staticmethod\ndef system_type():\n'''\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    '''            \n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/query/db-sql-connector/","title":"Databricks SQL Connector","text":""},{"location":"sdk/code-reference/query/db-sql-connector/#src.sdk.python.rtdip_sdk.odbc.db_sql_connector.DatabricksSQLConnection","title":"<code>DatabricksSQLConnection</code>","text":"<p>         Bases: <code>ConnectionInterface</code></p> <p>The Databricks SQL Connector for Python is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses. </p> <p>The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to intereact with cluster/jobs. To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.</p> <p>Parameters:</p> Name Type Description Default <code>server_hostname</code> <code>str</code> <p>Server hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD token</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/db_sql_connector.py</code> <pre><code>class DatabricksSQLConnection(ConnectionInterface):\n\"\"\"\n    The Databricks SQL Connector for Python is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses. \n\n    The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to intereact with cluster/jobs.\n    To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.\n\n    Args:\n        server_hostname: Server hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD token\n  \"\"\"\n  def __init__(self, server_hostname: str, http_path: str, access_token: str) -&gt; None:\n    #call auth method\n    self.connection = sql.connect(\n      server_hostname=server_hostname,\n      http_path=http_path,\n      access_token=access_token)\n\n  def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n    try:\n      self.connection.close()\n    except Exception as e:\n      logging.exception('error while closing connection')\n      raise e\n\n  def cursor(self) -&gt; object:\n\"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n      return DatabricksSQLCursor(self.connection.cursor())\n    except Exception as e:\n      logging.exception('error with cursor object')\n      raise e\n</code></pre>"},{"location":"sdk/code-reference/query/db-sql-connector/#src.sdk.python.rtdip_sdk.odbc.db_sql_connector.DatabricksSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/db_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n  try:\n    self.connection.close()\n  except Exception as e:\n    logging.exception('error while closing connection')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/db-sql-connector/#src.sdk.python.rtdip_sdk.odbc.db_sql_connector.DatabricksSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>DatabricksSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/db_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n  Intiates the cursor and returns it.\n\n  Returns:\n    DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n  \"\"\"\n  try:\n    return DatabricksSQLCursor(self.connection.cursor())\n  except Exception as e:\n    logging.exception('error with cursor object')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/db-sql-connector/#src.sdk.python.rtdip_sdk.odbc.db_sql_connector.DatabricksSQLCursor","title":"<code>DatabricksSQLCursor</code>","text":"<p>         Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/db_sql_connector.py</code> <pre><code>class DatabricksSQLCursor(CursorInterface):\n\"\"\"\n  Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n  Args:\n      cursor: controls execution of commands on cluster or SQL Warehouse\n  \"\"\"\n  def __init__(self, cursor: object) -&gt; None:\n    self.cursor = cursor\n\n  def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:  \n      self.cursor.execute(query)\n    except Exception as e:\n      logging.exception('error while executing the query')\n      raise e\n\n  def fetch_all(self) -&gt; list: \n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n      result = self.cursor.fetchall()\n      cols = [column[0] for column in self.cursor.description]\n      df = pd.DataFrame(result)\n      df.columns = cols\n      return df\n    except Exception as e:\n      logging.exception('error while fetching the rows of a query')\n      raise e\n\n  def close(self) -&gt; None: \n\"\"\"Closes the cursor.\"\"\"\n    try:\n      self.cursor.close()\n    except Exception as e:\n      logging.exception('error while closing the cursor')\n      raise e\n</code></pre>"},{"location":"sdk/code-reference/query/db-sql-connector/#src.sdk.python.rtdip_sdk.odbc.db_sql_connector.DatabricksSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/db_sql_connector.py</code> <pre><code>def close(self) -&gt; None: \n\"\"\"Closes the cursor.\"\"\"\n  try:\n    self.cursor.close()\n  except Exception as e:\n    logging.exception('error while closing the cursor')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/db-sql-connector/#src.sdk.python.rtdip_sdk.odbc.db_sql_connector.DatabricksSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/db_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n  Prepares and runs a database query.\n\n  Args:\n      query: sql query to execute on the cluster or SQL Warehouse\n  \"\"\"\n  try:  \n    self.cursor.execute(query)\n  except Exception as e:\n    logging.exception('error while executing the query')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/db-sql-connector/#src.sdk.python.rtdip_sdk.odbc.db_sql_connector.DatabricksSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/db_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list: \n\"\"\"\n  Gets all rows of a query.\n\n  Returns:\n      list: list of results\n  \"\"\"\n  try:\n    result = self.cursor.fetchall()\n    cols = [column[0] for column in self.cursor.description]\n    df = pd.DataFrame(result)\n    df.columns = cols\n    return df\n  except Exception as e:\n    logging.exception('error while fetching the rows of a query')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/interpolate/","title":"Interpolate Function","text":""},{"location":"sdk/code-reference/query/interpolate/#src.sdk.python.rtdip_sdk.functions.interpolate.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP interpolation function that is intertwined with the RTDIP Resampling function.</p> <p>The Interpolation function will forward fill or backward fill the resampled data depending users specified interpolation method.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below.)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security </p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)</p> <code>sample_rate</code> <code>int</code> <p>The resampling rate (numeric input)</p> <code>sample_unit</code> <code>str</code> <p>The resampling unit (second, minute, day, hour)</p> <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> <code>interpolation_method</code> <code>str</code> <p>Optional. Interpolation method (forward_fill, backward_fill)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>A resampled and interpolated dataframe.</p> Source code in <code>src/sdk/python/rtdip_sdk/functions/interpolate.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n'''\n    An RTDIP interpolation function that is intertwined with the RTDIP Resampling function.\n\n    The Interpolation function will forward fill or backward fill the resampled data depending users specified interpolation method.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below.)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security \n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)\n        sample_rate (int): The resampling rate (numeric input)\n        sample_unit (str): The resampling unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        interpolation_method (str): Optional. Interpolation method (forward_fill, backward_fill)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n\n    Returns:\n        DataFrame: A resampled and interpolated dataframe.\n    '''\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    try:\n        query = _query_builder(parameters_dict)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            return df\n        except Exception as e:\n            logging.exception('error returning dataframe')\n            raise e\n\n    except Exception as e:\n        logging.exception('error with interpolate function')\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/interpolate/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.authenticate import DefaultAuth\nfrom rtdip_sdk.odbc.db_sql_connector import DatabricksSQLConnection\nfrom rtdip_sdk.functions import interpolate\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\", \n    \"asset\": \"Asset Name\", \n    \"data_security_level\": \"Security Level\", \n    \"data_type\": \"float\", #options:[\"float\", \"double\", \"integer\", \"string\"]\n    \"tag_names\": [\"tag_1\", \"tag_2\"], #list of tags\n    \"start_date\": \"2023-01-01\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2023-01-31\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"sample_rate\": \"1\", #numeric input\n    \"sample_unit\": \"hour\", #options: [\"second\", \"minute\", \"day\", \"hour\"]\n    \"agg_method\": \"first\", #options: [\"first\", \"last\", \"avg\", \"min\", \"max\"]\n    \"interpolation_method\": \"forward_fill\", #options: [\"forward_fill\", \"backward_fill\"]\n    \"include_bad_data\": True, #options: [True, False]\n}\nx = interpolate.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code> or <code>TURBODBCSQLConnection()</code>.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/metadata/","title":"Metadata Function","text":""},{"location":"sdk/code-reference/query/metadata/#src.sdk.python.rtdip_sdk.functions.metadata.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back the metadata by querying databricks SQL Warehouse using a connection specified by the user. </p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset </p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>tag_names</code> <code>list</code> <p>(Optional) Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>A dataframe of metadata.</p> Source code in <code>src/sdk/python/rtdip_sdk/functions/metadata.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n'''\n    A function to return back the metadata by querying databricks SQL Warehouse using a connection specified by the user. \n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset \n        data_security_level (str): Level of data security\n        tag_names (list): (Optional) Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely\n\n    Returns:\n        DataFrame: A dataframe of metadata.\n    '''\n    try:\n        query = _query_builder(parameters_dict, metadata=True)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            return df\n        except Exception as e:\n            logging.exception('error returning dataframe')\n            raise e\n\n    except Exception as e:\n        logging.exception('error returning metadata function')\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/metadata/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.authenticate import DefaultAuth\nfrom rtdip_sdk.odbc.db_sql_connector import DatabricksSQLConnection\nfrom rtdip_sdk.functions import metadata\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\", \n    \"asset\": \"Asset Name\", \n    \"data_security_level\": \"Security Level\",\n    \"tag_names\": [\"tag_1\", \"tag_2\"], #list of tags\n}\nx = metadata.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code> or <code>TURBODBCSQLConnection()</code>.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/","title":"PYODBC SQL Connector","text":""},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#pyodbc-driver-paths","title":"PYODBC Driver Paths","text":"<p>To use PYODBC SQL Connect you will require the driver path specified below per operating system.</p> Operating Systems Driver Paths Windows <code>C:\\Program Files\\Simba Spark ODBC Driver</code> MacOS <code>/Library/simba/spark/lib/libsparkodbc_sbu.dylib</code> Linux 64-bit <code>/opt/simba/spark/lib/64/libsparkodbc_sb64.so</code> Linux 32-bit <code>/opt/simba/spark/lib/32/libsparkodbc_sb32.so</code>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.pyodbc_sql_connector.PYODBCSQLConnection","title":"<code>PYODBCSQLConnection</code>","text":"<p>         Bases: <code>ConnectionInterface</code></p> <p>PYODBC is an open source python module which allows access to ODBC databases.  This allows the user to connect through ODBC to data in azure databricks clusters or sql warehouses.</p> <p>Uses the databricks API's (2.0) to connect to the sql server.</p> <p>Parameters:</p> Name Type Description Default <code>driver_path</code> <code>str</code> <p>Driver installed to work with PYODBC</p> required <code>server_hostname</code> <code>str</code> <p>Server hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD Token</p> required Note 1 <p>More fields can be configured here in the connection ie PORT, Schema, etc.</p> Note 2 <p>When using Unix, Linux or Mac OS brew installation of PYODBC is required for connection.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/pyodbc_sql_connector.py</code> <pre><code>class PYODBCSQLConnection(ConnectionInterface):\n\"\"\"\n    PYODBC is an open source python module which allows access to ODBC databases. \n    This allows the user to connect through ODBC to data in azure databricks clusters or sql warehouses.\n\n    Uses the databricks API's (2.0) to connect to the sql server.\n\n    Args:\n        driver_path: Driver installed to work with PYODBC\n        server_hostname: Server hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD Token\n\n    Note 1:\n        More fields can be configured here in the connection ie PORT, Schema, etc.\n\n    Note 2:\n        When using Unix, Linux or Mac OS brew installation of PYODBC is required for connection.\n  \"\"\"\n  def __init__(self, driver_path: str, server_hostname: str, http_path: str, access_token: str) -&gt; None:\n\n    self.connection = pyodbc.connect('Driver=' + driver_path +';' +\n                                    'HOST=' + server_hostname + ';' +\n                                    'PORT=443;' +\n                                    'Schema=default;' +\n                                    'SparkServerType=3;' +\n                                    'AuthMech=11;' +\n                                    'UID=token;' +\n                                    #'PWD=' + access_token+ \";\" +\n                                    'Auth_AccessToken='+ access_token +';'\n                                    'ThriftTransport=2;' +\n                                    'SSL=1;' +\n                                    'HTTPPath=' + http_path,\n                                    autocommit=True)\n\n  def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n    try:\n      self.connection.close()\n    except Exception as e:\n      logging.exception('error while closing the connection')\n      raise e\n\n  def cursor(self) -&gt; object:\n\"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      PYODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n      return PYODBCSQLCursor(self.connection.cursor())\n    except Exception as e:\n      logging.exception('error with cursor object')\n      raise e\n</code></pre>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.pyodbc_sql_connector.PYODBCSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/pyodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n  try:\n    self.connection.close()\n  except Exception as e:\n    logging.exception('error while closing the connection')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.pyodbc_sql_connector.PYODBCSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>PYODBCSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/pyodbc_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n  Intiates the cursor and returns it.\n\n  Returns:\n    PYODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n  \"\"\"\n  try:\n    return PYODBCSQLCursor(self.connection.cursor())\n  except Exception as e:\n    logging.exception('error with cursor object')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.pyodbc_sql_connector.PYODBCSQLCursor","title":"<code>PYODBCSQLCursor</code>","text":"<p>         Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/pyodbc_sql_connector.py</code> <pre><code>class PYODBCSQLCursor(CursorInterface):\n\"\"\"\n  Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n  Args:\n      cursor: controls execution of commands on cluster or SQL Warehouse\n  \"\"\"\n  def __init__(self, cursor: object) -&gt; None:\n    self.cursor = cursor\n\n  def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n      self.cursor.execute(query)\n\n    except Exception as e:\n      logging.exception('error while executing the query')\n      raise e\n\n  def fetch_all(self) -&gt; list: \n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n      result = self.cursor.fetchall()\n      cols = [column[0] for column in self.cursor.description]\n      result = [list(x) for x in result]\n      df = pd.DataFrame(result)\n      df.columns = cols\n      return df\n    except Exception as e:\n      logging.exception('error while fetching rows from the query')\n      raise e\n\n  def close(self) -&gt; None: \n\"\"\"Closes the cursor.\"\"\"\n    try:\n      self.cursor.close()\n    except Exception as e:\n      logging.exception('error while closing the cursor')\n      raise e\n</code></pre>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.pyodbc_sql_connector.PYODBCSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/pyodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None: \n\"\"\"Closes the cursor.\"\"\"\n  try:\n    self.cursor.close()\n  except Exception as e:\n    logging.exception('error while closing the cursor')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.pyodbc_sql_connector.PYODBCSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/pyodbc_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n  Prepares and runs a database query.\n\n  Args:\n      query: sql query to execute on the cluster or SQL Warehouse\n  \"\"\"\n  try:\n    self.cursor.execute(query)\n\n  except Exception as e:\n    logging.exception('error while executing the query')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.pyodbc_sql_connector.PYODBCSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/pyodbc_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list: \n\"\"\"\n  Gets all rows of a query.\n\n  Returns:\n      list: list of results\n  \"\"\"\n  try:\n    result = self.cursor.fetchall()\n    cols = [column[0] for column in self.cursor.description]\n    result = [list(x) for x in result]\n    df = pd.DataFrame(result)\n    df.columns = cols\n    return df\n  except Exception as e:\n    logging.exception('error while fetching rows from the query')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/raw/","title":"Raw Function","text":""},{"location":"sdk/code-reference/query/raw/#src.sdk.python.rtdip_sdk.functions.raw.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user. </p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit </p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset </p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/functions/raw.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n'''\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user. \n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit \n        region (str): Region\n        asset (str): Asset \n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    '''\n    try:\n        query = _query_builder(parameters_dict)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            if df['EventTime'][0].tzinfo is None:\n                df['EventTime'] = df['EventTime'].apply(lambda x: x.replace(tzinfo=pytz.timezone(\"Etc/UTC\")))\n            cursor.close()\n            return df\n        except Exception as e:\n            logging.exception('error returning dataframe')\n            raise e\n\n    except Exception as e:\n        logging.exception('error with raw function')\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/raw/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.authenticate import DefaultAuth\nfrom rtdip_sdk.odbc.db_sql_connector import DatabricksSQLConnection\nfrom rtdip_sdk.functions import raw\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\", \n    \"asset\": \"Asset Name\", \n    \"data_security_level\": \"Security Level\", \n    \"data_type\": \"float\", #options:[\"float\", \"double\", \"integer\", \"string\"]\n    \"tag_names\": [\"tag_1\", \"tag_2\"], #list of tags\n    \"start_date\": \"2023-01-01\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2023-01-31\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"include_bad_data\": True, #options: [True, False]\n}\nx = raw.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code> or <code>TURBODBCSQLConnection()</code>. </p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/resample/","title":"Resample Function","text":""},{"location":"sdk/code-reference/query/resample/#src.sdk.python.rtdip_sdk.functions.resample.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP Resampling function in spark to resample data by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)</p> <code>sample_rate</code> <code>int</code> <p>The resampling rate (numeric input)</p> <code>sample_unit</code> <code>str</code> <p>The resampling unit (second, minute, day, hour)</p> <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>A resampled dataframe.</p> Source code in <code>src/sdk/python/rtdip_sdk/functions/resample.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n'''\n    An RTDIP Resampling function in spark to resample data by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS)\n        sample_rate (int): The resampling rate (numeric input)\n        sample_unit (str): The resampling unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n\n    Returns:\n        DataFrame: A resampled dataframe.\n    '''\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    try:\n        query = _query_builder(parameters_dict)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            return df\n        except Exception as e:\n            logging.exception('error returning dataframe')\n            raise e\n\n    except Exception as e:\n        logging.exception('error with resampling function')\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/resample/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.authenticate import DefaultAuth\nfrom rtdip_sdk.odbc.db_sql_connector import DatabricksSQLConnection\nfrom rtdip_sdk.functions import resample\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\", \n    \"asset\": \"Asset Name\", \n    \"data_security_level\": \"Security Level\", \n    \"data_type\": \"float\", #options:[\"float\", \"double\", \"integer\", \"string\"]\n    \"tag_names\": [\"tag_1\", \"tag_2\"], #list of tags\n    \"start_date\": \"2023-01-01\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2023-01-31\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"sample_rate\": \"1\", #numeric input\n    \"sample_unit\": \"hour\", #options: [\"second\", \"minute\", \"day\", \"hour\"]\n    \"agg_method\": \"first\", #options: [\"first\", \"last\", \"avg\", \"min\", \"max\"]\n    \"include_bad_data\": True, #options: [True, False]\n}\nx = resample.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code> or <code>TURBODBCSQLConnection()</code>.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/time-weighted-average/","title":"Time Weighted Average","text":""},{"location":"sdk/code-reference/query/time-weighted-average/#src.sdk.python.rtdip_sdk.functions.time_weighted_average.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function that recieves a dataframe of raw tag data and performs a timeweighted average, returning the results. </p> <p>This function requires the input of a pandas dataframe acquired via the rtdip.functions.raw() method and the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Pi data points will either have step enabled (True) or step disabled (False). You can specify whether you want step to be fetched by \"Pi\" or you can set the step parameter to True/False in the dictionary below.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit </p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset </p> <code>data_security_level</code> <code>str</code> <p>Level of data security </p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames</p> <code>start_date</code> <code>str</code> <p>Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS)</p> <code>end_date</code> <code>str</code> <p>End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS)</p> <code>window_size_mins</code> <code>int</code> <p>Window size in minutes</p> <code>window_length</code> <code>int</code> <p>(Optional) add longer window time for the start or end of specified date to cater for edge cases</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>step</code> <code>str</code> <p>data points with step \"enabled\" or \"disabled\". The options for step are \"metadata\" (string), True or False (bool)</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>A dataframe containing the time weighted averages.</p> Source code in <code>src/sdk/python/rtdip_sdk/functions/time_weighted_average.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n'''\n    A function that recieves a dataframe of raw tag data and performs a timeweighted average, returning the results. \n\n    This function requires the input of a pandas dataframe acquired via the rtdip.functions.raw() method and the user to input a dictionary of parameters. (See Attributes table below)\n\n    Pi data points will either have step enabled (True) or step disabled (False). You can specify whether you want step to be fetched by \"Pi\" or you can set the step parameter to True/False in the dictionary below.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict (dict): A dictionary of parameters (see Attributes table below)\n    Attributes:\n        business_unit (str): Business unit \n        region (str): Region\n        asset (str): Asset \n        data_security_level (str): Level of data security \n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames\n        start_date (str): Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS)\n        end_date (str): End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS)\n        window_size_mins (int): Window size in minutes\n        window_length (int): (Optional) add longer window time for the start or end of specified date to cater for edge cases\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        step (str): data points with step \"enabled\" or \"disabled\". The options for step are \"metadata\" (string), True or False (bool)\n    Returns:\n        DataFrame: A dataframe containing the time weighted averages.\n    '''\n    try:\n        datetime_format = \"%Y-%m-%dT%H:%M:%S\"\n        utc=\"Etc/UTC\"\n        if len(parameters_dict[\"start_date\"]) == 10:\n            original_start_date = datetime.strptime(parameters_dict[\"start_date\"] + \"T00:00:00\", datetime_format)\n            parameters_dict[\"start_date\"] = parameters_dict[\"start_date\"] + \"T00:00:00\"\n        else:\n            original_start_date = datetime.strptime(parameters_dict[\"start_date\"], datetime_format)\n        if len(parameters_dict[\"end_date\"]) == 10:\n            original_end_date = datetime.strptime(parameters_dict[\"end_date\"] + \"T23:59:59\", datetime_format) \n            parameters_dict[\"end_date\"] = parameters_dict[\"end_date\"] + \"T23:59:59\"\n        else: \n            original_end_date = datetime.strptime(parameters_dict[\"end_date\"], datetime_format)\n        if \"window_length\" in parameters_dict:       \n            parameters_dict[\"start_date\"] = (datetime.strptime(parameters_dict[\"start_date\"], datetime_format) - timedelta(minutes = int(parameters_dict[\"window_length\"]))).strftime(datetime_format)\n            parameters_dict[\"end_date\"] = (datetime.strptime(parameters_dict[\"end_date\"], datetime_format) + timedelta(minutes = int(parameters_dict[\"window_length\"]))).strftime(datetime_format) \n        else:\n            parameters_dict[\"start_date\"] = (datetime.strptime(parameters_dict[\"start_date\"], datetime_format) - timedelta(minutes = int(parameters_dict[\"window_size_mins\"]))).strftime(datetime_format)\n            parameters_dict[\"end_date\"] = (datetime.strptime(parameters_dict[\"end_date\"], datetime_format) + timedelta(minutes = int(parameters_dict[\"window_size_mins\"]))).strftime(datetime_format)\n        pandas_df = raw_get(connection, parameters_dict)\n\n        pandas_df[\"EventDate\"] = pd.to_datetime(pandas_df[\"EventTime\"]).dt.date  \n        boundaries_df = pd.DataFrame(columns=[\"EventTime\", \"TagName\"])\n        for tag in parameters_dict[\"tag_names\"]:\n            start_date_new_row = pd.DataFrame([[pd.to_datetime(parameters_dict[\"start_date\"]).replace(tzinfo=pytz.timezone(utc)), tag]], columns=[\"EventTime\", \"TagName\"])\n            end_date_new_row = pd.DataFrame([[pd.to_datetime(parameters_dict[\"end_date\"]).replace(tzinfo=pytz.timezone(utc)), tag]], columns=[\"EventTime\", \"TagName\"])\n            boundaries_df = pd.concat([boundaries_df, start_date_new_row, end_date_new_row], ignore_index=True)\n        boundaries_df.set_index(pd.DatetimeIndex(boundaries_df[\"EventTime\"]), inplace=True)\n        boundaries_df.drop(columns=\"EventTime\", inplace=True)\n        boundaries_df = boundaries_df.groupby([\"TagName\"]).resample(\"{}T\".format(str(parameters_dict[\"window_size_mins\"]))).ffill().drop(columns='TagName')\n        #preprocess - add boundaries and time interpolate missing boundary values\n        preprocess_df = pandas_df.copy()\n        preprocess_df[\"EventTime\"] = preprocess_df[\"EventTime\"].round(\"S\")\n        preprocess_df.set_index([\"EventTime\", \"TagName\", \"EventDate\"], inplace=True)\n        preprocess_df = preprocess_df.join(boundaries_df, how=\"outer\", rsuffix=\"right\")\n        if isinstance(parameters_dict[\"step\"], str) and parameters_dict[\"step\"].lower() == \"metadata\":\n            metadata_df = metadata_get(connection, parameters_dict)\n            metadata_df.set_index(\"TagName\", inplace=True)\n            metadata_df = metadata_df.loc[:, \"Step\"]\n            preprocess_df = preprocess_df.merge(metadata_df, left_index=True, right_index=True)\n        elif parameters_dict[\"step\"] == True:\n            preprocess_df[\"Step\"] =  True\n        elif parameters_dict[\"step\"] == False:\n            preprocess_df[\"Step\"] = False\n        else:\n            raise Exception('Unexpected step value', parameters_dict[\"step\"])\n        def process_time_weighted_averages_step(pandas_df):\n            if pandas_df[\"Step\"].any() == False:\n                pandas_df = pandas_df.reset_index(level=[\"TagName\", \"EventDate\"]).sort_index().interpolate(method='time')\n                shift_raw_df = pandas_df.copy()\n                shift_raw_df[\"CalcValue\"] = (shift_raw_df.index.to_series().diff().dt.seconds/86400) * shift_raw_df.Value.rolling(2).sum()\n                time_weighted_averages = shift_raw_df.resample(\"{}T\".format(str(parameters_dict[\"window_size_mins\"])), closed=\"right\", label=\"right\").CalcValue.sum() * 0.5 / parameters_dict[\"window_size_mins\"] * 24 * 60\n                return time_weighted_averages\n            else:\n                pandas_df = pandas_df.reset_index(level=[\"TagName\", \"EventDate\"]).sort_index().interpolate(method='pad', limit_direction='forward')\n                shift_raw_df = pandas_df.copy()\n                shift_raw_df[\"CalcValue\"] = (shift_raw_df.index.to_series().diff().dt.seconds/86400) * shift_raw_df.Value.shift(1)\n                time_weighted_averages = shift_raw_df.resample(\"{}T\".format(str(parameters_dict[\"window_size_mins\"])), closed=\"right\", label=\"right\").CalcValue.sum() / parameters_dict[\"window_size_mins\"] * 24 * 60\n                return time_weighted_averages\n\n        #calculate time weighted averages\n        time_weighted_averages = preprocess_df.groupby([\"TagName\"]).apply(process_time_weighted_averages_step).reset_index()\n\n        if \"CalcValue\" not in time_weighted_averages.columns:\n            time_weighted_averages = time_weighted_averages.melt(id_vars=\"TagName\", var_name=\"EventTime\", value_name=\"Value\")\n        else: \n            time_weighted_averages = time_weighted_averages.rename(columns={\"CalcValue\": \"Value\"})\n\n        time_weighted_averages = time_weighted_averages.set_index(\"EventTime\").sort_values(by=[\"TagName\", \"EventTime\"])\n\n        time_weighted_averages_datetime = time_weighted_averages.index.to_pydatetime()\n        weighted_averages_timezones = np.array([z.replace(tzinfo=pytz.timezone(utc)) for z in time_weighted_averages_datetime])\n        time_weighted_averages = time_weighted_averages[(original_start_date.replace(tzinfo=pytz.timezone(utc)) &lt; weighted_averages_timezones) &amp; (weighted_averages_timezones &lt;= original_end_date.replace(tzinfo=pytz.timezone(utc)) + timedelta(seconds = 1))]\n        return time_weighted_averages\n\n\n    except Exception as e:\n        logging.exception('error with time weighted average function', str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/time-weighted-average/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.authenticate import DefaultAuth\nfrom rtdip_sdk.odbc.db_sql_connector import DatabricksSQLConnection\nfrom rtdip_sdk.functions import time_weighted_average\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\", \n    \"asset\": \"Asset Name\", \n    \"data_security_level\": \"Security Level\", \n    \"data_type\": \"float\", #options:[\"float\", \"double\", \"integer\", \"string\"]\n    \"tag_names\": [\"tag_1\", \"tag_2\"], #list of tags\n    \"start_date\": \"2023-01-01\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2023-01-31\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"window_size_mins\": 15, #numeric input\n    \"window_length\": 20, #numeric input\n    \"include_bad_data\": True, #options: [True, False]\n    \"step\": True\n}\nx = time_weighted_average.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code> or <code>TURBODBCSQLConnection()</code>.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/turbodbc-sql-connector/","title":"TURBODBC SQL Connector","text":""},{"location":"sdk/code-reference/query/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.turbodbc_sql_connector.TURBODBCSQLConnection","title":"<code>TURBODBCSQLConnection</code>","text":"<p>         Bases: <code>ConnectionInterface</code></p> <p>Turbodbc is a python module used to access relational databases through an ODBC interface. It will allow a user to connect to databricks clusters or sql warehouses.</p> <p>Turbodbc offers built-in NumPy support allowing it to be much faster for processing compared to other connectors. To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.</p> <p>Parameters:</p> Name Type Description Default <code>server_hostname</code> <code>str</code> <p>hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD Token</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/turbodbc_sql_connector.py</code> <pre><code>class TURBODBCSQLConnection(ConnectionInterface):\n\"\"\"\n  Turbodbc is a python module used to access relational databases through an ODBC interface. It will allow a user to connect to databricks clusters or sql warehouses.\n\n  Turbodbc offers built-in NumPy support allowing it to be much faster for processing compared to other connectors.\n  To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.\n\n  Args:\n      server_hostname: hostname for the cluster or SQL Warehouse\n      http_path: Http path for the cluster or SQL Warehouse\n      access_token: Azure AD Token\n\n  Note: \n      More fields such as driver can be configured upon extension.\n  \"\"\"\n  def __init__(self, server_hostname: str, http_path: str, access_token: str) -&gt; None:\n    options = make_options(\n        autocommit=True, \n        read_buffer_size=Megabytes(100),\n        use_async_io=True)\n    self.connection = connect(Driver=\"Simba Spark ODBC Driver\",\n                              Server=server_hostname,\n                              HOST=server_hostname,\n                              PORT=443,\n                              SparkServerType=3,\n                              Schema=\"default\",\n                              ThriftTransport=2,\n                              SSL=1,\n                              AuthMech=11,\n                              Auth_AccessToken=access_token,\n                              Auth_Flow=0,\n                              HTTPPath=http_path,\n                              UseNativeQuery=1,\n                              FastSQLPrepare=1,\n                              ApplyFastSQLPrepareToAllQueries=1,\n                              DisableLimitZero=1,                      \n                              EnableAsyncExec=1,\n                              turbodbc_options=options)\n\n  def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n    try:\n      self.connection.close()\n    except Exception as e:\n      logging.exception('error while closing the connection')\n      raise e\n\n  def cursor(self) -&gt; object:\n\"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      TURBODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n      return TURBODBCSQLCursor(self.connection.cursor())\n    except Exception as e:\n      logging.exception('error with cursor object')\n      raise e\n</code></pre>"},{"location":"sdk/code-reference/query/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.turbodbc_sql_connector.TURBODBCSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/turbodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n  try:\n    self.connection.close()\n  except Exception as e:\n    logging.exception('error while closing the connection')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.turbodbc_sql_connector.TURBODBCSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>TURBODBCSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/turbodbc_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n  Intiates the cursor and returns it.\n\n  Returns:\n    TURBODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n  \"\"\"\n  try:\n    return TURBODBCSQLCursor(self.connection.cursor())\n  except Exception as e:\n    logging.exception('error with cursor object')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.turbodbc_sql_connector.TURBODBCSQLCursor","title":"<code>TURBODBCSQLCursor</code>","text":"<p>         Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/turbodbc_sql_connector.py</code> <pre><code>class TURBODBCSQLCursor(CursorInterface):\n\"\"\"\n  Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n  Args:\n      cursor: controls execution of commands on cluster or SQL Warehouse\n  \"\"\"  \n  def __init__(self, cursor: object) -&gt; None:\n    self.cursor = cursor\n\n  def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n      self.cursor.execute(query)\n    except Exception as e:\n      logging.exception('error while executing the query')\n      raise e\n\n  def fetch_all(self) -&gt; list: \n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n      result = self.cursor.fetchall()\n      cols = [column[0] for column in self.cursor.description]\n      df = pd.DataFrame(result)\n      df.columns = cols\n      return df\n    except Exception as e:\n      logging.exception('error while fetching the rows from the query')\n      raise e\n\n  def close(self) -&gt; None: \n\"\"\"Closes the cursor.\"\"\"\n    try:\n      self.cursor.close()\n    except Exception as e:\n      logging.exception('error while closing the cursor')\n      raise e\n</code></pre>"},{"location":"sdk/code-reference/query/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/turbodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None: \n\"\"\"Closes the cursor.\"\"\"\n  try:\n    self.cursor.close()\n  except Exception as e:\n    logging.exception('error while closing the cursor')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/odbc/turbodbc_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n  Prepares and runs a database query.\n\n  Args:\n      query: sql query to execute on the cluster or SQL Warehouse\n  \"\"\"\n  try:\n    self.cursor.execute(query)\n  except Exception as e:\n    logging.exception('error while executing the query')\n    raise e\n</code></pre>"},{"location":"sdk/code-reference/query/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/odbc/turbodbc_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list: \n\"\"\"\n  Gets all rows of a query.\n\n  Returns:\n      list: list of results\n  \"\"\"\n  try:\n    result = self.cursor.fetchall()\n    cols = [column[0] for column in self.cursor.description]\n    df = pd.DataFrame(result)\n    df.columns = cols\n    return df\n  except Exception as e:\n    logging.exception('error while fetching the rows from the query')\n    raise e\n</code></pre>"},{"location":"sdk/pipelines/components/","title":"Pipeline Components","text":""},{"location":"sdk/pipelines/components/#overview","title":"Overview","text":"<p>The Real Time Data Ingestion Pipeline Framework supports the following component types:</p> <ul> <li>Sources - connectors to source systems</li> <li>Transformers - perform transformations on data, including data cleansing, data enrichment, data aggregation, data masking, data encryption, data decryption, data validation, data conversion, data normalization, data de-normalization, data partitioning etc</li> <li>Destinations - connectors to sink/destination systems </li> <li>Utilities - components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance etc</li> <li>Secrets - components that facilitate accessing secret stores where sensitive information is stored such as passwords, connectiong strings, keys etc</li> </ul>"},{"location":"sdk/pipelines/components/#component-types","title":"Component Types","text":"Python Apache Spark Databricks <p>Component Types determine system requirements to execute the component:</p> <ul> <li>Python - components that are written in python and can be executed on a python runtime</li> <li>Pyspark - components that are written in pyspark can be executed on an open source Apache Spark runtime</li> <li>Databricks - components that require a Databricks runtime</li> </ul> <p>Note</p> <p>RTDIP are continuously adding more to this list. For detailed information on timelines, read this blog post and check back on this page regularly</p>"},{"location":"sdk/pipelines/components/#sources","title":"Sources","text":"<p>Sources are components that connect to source systems and extract data from them. These will typically be real time data sources, but also support batch components as these are still important and necessary data souces of time series data in a number of circumstances in the real world.</p> Source Type Python Apache Spark Databricks Azure AWS Delta Delta Sharing Autoloader Eventhub <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#transformers","title":"Transformers","text":"<p>Transformers are components that perform transformations on data. These will target certain data models and common transformations that sources or destination components require to be performed on data before it can be ingested or consumed.</p> Transformer Type Python Apache Spark Databricks Azure AWS Eventhub Body <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#destinations","title":"Destinations","text":"<p>Destinations are components that connect to sink/destination systems and write data to them. </p> Destination Type Python Apache Spark Databricks Azure AWS Delta Append Eventhub <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#utilities","title":"Utilities","text":"<p>Utilities are components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance and are normally components that can be executed as part of a pipeline or standalone.</p> Utility Type Python Apache Spark Databricks Azure AWS Delta Table Create <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#secrets","title":"Secrets","text":"<p>Secrets are components that perform functions to interact with secret stores to manage sensitive information such as passwords, keys and certificates.</p> Secret Type Python Apache Spark Databricks Azure AWS Databricks Secret Scopes <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#conclusion","title":"Conclusion","text":"<p>Components can be used to build RTDIP Pipelines which is described in more detail here.</p>"},{"location":"sdk/pipelines/framework/","title":"RTDIP Ingestion Pipeline Framework","text":"<p>RTDIP has been built to simplify ingesting and querying time series data. The RTDIP Ingestion Pipeline Framework creates streaming and batch ingestion pipelines according to requirements of the source of the data and needs of the data consumer. RTDIP Pipelines focuses on the ingestion of data into the platform.</p>"},{"location":"sdk/pipelines/framework/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have followed the installation instructions as specified in the Getting Started section and follow the steps which highlight the installation requirements for Pipelines. In particular:</p> <ol> <li>RTDIP SDK Installation</li> <li>Java - If your pipeline steps utilize pyspark then Java must be installed.</li> </ol> <p>RTDIP SDK installation</p> <p>Ensure you have installed the RTDIP SDK, as a minimum, as follows: <pre><code>pip install rtdip-sdk[pipelines]\n</code></pre></p> <p>For all installation options please see the RTDIP SDK installation instructions.</p>"},{"location":"sdk/pipelines/framework/#overview","title":"Overview","text":"<p>The goal of the RTDIP Ingestion Pipeline framework is to:</p> <ol> <li>Support python and pyspark to build pipeline components</li> <li>Enable execution of sources, transformers, destinations and utilities components in a framework that can execute them in a defined order</li> <li>Create modular components that can be leveraged as a step in a pipeline task using Object Oriented Programming techniques included Interfaces and Implementations per component type</li> <li>Deploy pipelines to popular orchestration engines</li> <li>Ensure pipelines can be constructed and executed using the RTDIP SDK and rest APIs</li> </ol>"},{"location":"sdk/pipelines/framework/#jobs","title":"Jobs","text":"<p>The RTDIP Data Ingestion Pipeline Framework follow sthe typical convention of a job that users will be familiar with if they have used orchestration engines such as Apache Airflow or Databricks Workflows.</p> <p>A pipline job consists of the following components:</p> <pre><code>erDiagram\n  JOB ||--|{ TASK : contains\n  TASK ||--|{ STEP : contains\n  JOB {\n    string name\n    string description\n    list task_list\n  }\n  TASK {\n    string name\n    string description\n    string depends_on_task\n    list step_list\n    bool batch_task\n  }\n  STEP {\n    string name\n    string description\n    list depends_on_step\n    list provides_output_to_step\n    class component\n    dict component_parameters\n  }</code></pre> <p>As per the above, a pipeline job consists of a list of tasks. Each task consists of a list of steps. Each step consists of a component and a set of parameters that are passed to the component. Dependency Injection will ensure that each component is instantiated with the correct parameters. </p> <p>More Information about Pipeline Jobs can be found here.</p>"},{"location":"sdk/pipelines/framework/#runtime-environments","title":"Runtime Environments","text":"Python Apache Spark Databricks Delta Live Tables <p>Note</p> <p>RTDIP are continuously adding more to this list. For detailed information on timelines, read this blog post and check back on this page regularly</p> <p>Pipelines can run in multiple environment types. These include:</p> <ul> <li>Python: Components written in python and executed on a python runtime</li> <li>Pyspark: Components written in pyspark and executed on an open source Apache Spark runtime</li> <li>Databricks: Components written in pyspark and executed on a Databricks runtime</li> <li>Delta Live Tables: Components written in pyspark and executed on a Databricks Delta Live Tables runtime</li> </ul> <p>Runtimes will take precedence depending on the list of components in a pipeline task.</p> <ul> <li>Pipelines with at least one Databricks or DLT component will be executed in a Databricks environment</li> <li>Pipelines with at least one Pyspark component will be executed in a Pyspark environment</li> <li>Pipelines with only Python components will be executed in a Python environment</li> </ul>"},{"location":"sdk/pipelines/framework/#conclusion","title":"Conclusion","text":"<p>Find out more about the components that can be used by the RTDIP Ingestion Pipeline Framework here.</p>"},{"location":"sdk/pipelines/jobs/","title":"Jobs","text":"<p>In a production environment, pipelines will be run as jobs that are either batch jobs executed on a schedule or a streaming job executed to be run continuously. </p>"},{"location":"sdk/pipelines/jobs/#build-a-pipeline","title":"Build a Pipeline","text":""},{"location":"sdk/pipelines/jobs/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have followed the installation instructions as specified in the Getting Started section and follow the steps which highlight the installation requirements for Pipelines. In particular:</p> <ol> <li>RTDIP SDK Installation</li> <li>Java - If your pipeline steps utilize pyspark then Java must be installed.</li> </ol> <p>RTDIP SDK installation</p> <p>Ensure you have installed the RTDIP SDK, as a minimum, as follows: <pre><code>pip install rtdip-sdk[pipelines]\n</code></pre></p> <p>For all installation options please see the RTDIP SDK installation instructions.</p>"},{"location":"sdk/pipelines/jobs/#import","title":"Import","text":"<p>Import the required components of a Pipeline Job.</p> <pre><code>from rtdip_sdk.pipelines.execute import PipelineJob, PipelineStep, PipelineTask\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.transformers import EventhubBodyBinaryToString\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\nfrom rtdip_sdk.pipelines.secrets import PipelineSecret, DatabricksSecrets\n</code></pre>"},{"location":"sdk/pipelines/jobs/#steps","title":"Steps","text":"<p>Pipeline steps are constructed from components and added to a Pipeline task as a list. Each component is created as a <code>PipelineStep</code> and populated with the following information.</p> Parameter Description Requirements Name Each component requires a unique name that also facilitates dependencies between each component Contains only letters, numbers and underscores Description A brief description of each component Will populate certain components of a runtime such as Delta Live Tables Component The component Class Populate with the Class Name Component Parameters Configures the component with specific information, such as connection information and component specific settings Use Pipeline Secrets for sensitive Information Depends On Step Specifies any component names that must be executed prior to this component A python list of component names Provides Output To Step Specifiers any component names that require this components output as an input A python list of component names <pre><code>step_list = []\n\n# read step\neventhub_configuration = {\n    \"eventhubs.connectionString\": PipelineSecret(type=DatabricksSecrets, vault=\"test_vault\", key=\"test_key\")\n    \"eventhubs.consumerGroup\": \"$Default\",\n    \"eventhubs.startingPosition\": {\"offset\": \"0\", \"seqNo\": -1, \"enqueuedTime\": None, \"isInclusive\": True}\n}    \nstep_list.append(PipelineStep(\n    name=\"test_step1\",\n    description=\"test_step1\",\n    component=SparkEventhubSource,\n    component_parameters={\"options\": eventhub_configuration},\n    provide_output_to_step=[\"test_step2\"]\n))\n\n# transform step\nstep_list.append(PipelineStep(\n    name=\"test_step2\",\n    description=\"test_step2\",\n    component=EventhubBodyBinaryToString,\n    component_parameters={},\n    depends_on_step=[\"test_step1\"],\n    provide_output_to_step=[\"test_step3\"]\n))\n\n# write step\nstep_list.append(PipelineStep(\n    name=\"test_step3\",\n    description=\"test_step3\",\n    component=SparkDeltaDestination,\n    component_parameters={\n        \"table_name\": \"test_table\",\n        \"options\": {},\n        \"mode\": \"overwrite\"    \n    },\n    depends_on_step=[\"test_step2\"]\n))\n</code></pre>"},{"location":"sdk/pipelines/jobs/#tasks","title":"Tasks","text":"<p>Tasks contain a list of steps. Each task is created as a <code>PipelineTask</code> and populated with the following information.</p> Parameter Description Requirements Name Each task requires a unique name Contains only letters, numbers and underscores Description A brief description of the task Will populate certain components of a runtime such as Delta Live Tables Step List A python list of steps that are to be executed by the task A list of step names that contain only letters, numbers and underscores Batch Task The task should be executed as a batch task Optional, defaults to False <pre><code>task = PipelineTask(\n    name=\"test_task\",\n    description=\"test_task\",\n    step_list=step_list,\n    batch_task=True\n)\n</code></pre>"},{"location":"sdk/pipelines/jobs/#jobs_1","title":"Jobs","text":"<p>Jobs contain a list of tasks. A job is created as a <code>PipelineJob</code> and populated with the following information.</p> Parameter Description Requirements Name The Job requires a unique name Contains only letters, numbers and underscores Description A brief description of the job Will populate certain components of a runtime such as Delta Live Tables Version Enables version control of the task for certain environments Follow semantic versioning Task List A python list of tasks that are to be executed by the  job A list of task names that contain only letters, numbers and underscores <pre><code>pipeline_job = PipelineJob(\n    name=\"test_job\",\n    description=\"test_job\", \n    version=\"0.0.1\",\n    task_list=[task]\n)\n</code></pre>"},{"location":"sdk/pipelines/jobs/#execute","title":"Execute","text":"<p>Pipeline Jobs can be executed directly if the run environment where the code has been written facilitates it. To do so, the above Pipeline Job can be executed as follows:</p> <pre><code>from python.rtdip_sdk.pipelines.execute import PipelineJobExecute\n\npipeline = PipelineJobExecute(pipeline_job)\n\nresult = pipeline.run()\n</code></pre>"},{"location":"sdk/pipelines/jobs/#conclusion","title":"Conclusion","text":"<p>The above sets out how a Pipeline Job can be constructed and executed. Most pipelines, however, will be exevcuted by orchestration engines. See the Deploy section for more information above how Pipeline Jobs can be deployed and executed in this way.</p>"},{"location":"sdk/pipelines/deploy/apache-airflow/","title":"Apache Airflow","text":""},{"location":"sdk/pipelines/deploy/apache-airflow/#databricks-provider","title":"Databricks Provider","text":"<p>Apache Airflow can orchestrate an RTDIP Pipeline that has been deployed as a Databricks Job. For further information on how to deploy an RTDIP Pipeline as a Databricks Job, please see here. </p> <p>Databricks has also provided more information about running Databricks jobs from Apache Airflow here.</p>"},{"location":"sdk/pipelines/deploy/apache-airflow/#prerequisites","title":"Prerequisites","text":"<ol> <li>An Apache Airflow instance must be running.</li> <li>Authentication between Apache Airflow and Databricks must be configured.</li> <li>The python packages <code>apache-airflow</code> and <code>apache-airflow-providers-databricks</code> must be installed.</li> <li>You have created an RTDIP Pipeline and deployed it to Databricks.</li> </ol>"},{"location":"sdk/pipelines/deploy/apache-airflow/#example","title":"Example","text":"<p>The <code>JOB ID</code> in the example below can be obtained from the Databricks Job.</p> <pre><code>from airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n  'owner': 'airflow'\n}\n\nwith DAG('databricks_dag',\n  start_date = days_ago(2),\n  schedule_interval = None,\n  default_args = default_args\n  ) as dag:\n\n  opr_run_now = DatabricksRunNowOperator(\n    task_id = 'run_now',\n    databricks_conn_id = 'databricks_default',\n    job_id = JOB_ID\n  )\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/","title":"Databricks Workflows","text":""},{"location":"sdk/pipelines/deploy/databricks/#import","title":"Import","text":"<pre><code>from rtdip_sdk.pipelines.deploy import DatabricksDBXDeploy, DatabricksCluster, DatabricksJobCluster, DatabricksJobForPipelineJob, DatabricksTaskForPipelineTask\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#authentication","title":"Authentication","text":"Azure Active DirectoryDatabricks <p>Refer to the Azure Active Directory documentation for further options to perform Azure AD authentication, such as Service Principal authentication using certificates or secrets. Below is an example of performing default authentication that retrieives a token for Azure Databricks. </p> <p>Also refer to the Code Reference for further technical information.</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>If you are experiencing any trouble authenticating please see Troubleshooting - Authentication</p> <p>Refer to the Databricks documentation for further information about generating a Databricks PAT Token. Below is an example of performing default authentication that retrieives a token for a Databricks Workspace. </p> <p>Provide your <code>dbapi.....</code> token to the <code>access_token</code> in the examples below.</p> <pre><code>access_token = \"dbapi..........\"\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#deploy","title":"Deploy","text":"<p>Deployments to Databricks are done using DBX. DBX enables users to control exactly how they deploy their RTDIP Pipelines to Databricks. In The RTDIP SDK package, all Databricks Deployment </p> <p>Any of the Classes below can be imported from the following location:</p> <pre><code>from rtdip_sdk.pipelines.deploy import *\n</code></pre> <p>Parameters for a Databricks Job can be managed using the following Classes:</p> Class Description DatabricksCluster Provides Parameters for setting up a Databricks Cluster DatabricksJobCluster Sets up a Jobs Cluster as defined by the provided <code>DatabricksCluster</code> DatabricksTask Defines the setup of the Task at the Databricks Task level including Task specific Clusters, Libraries, Schedules, Notifications and Timeouts DatabricksJob Defines the setup at the Job level including Clusters, Libraries, Schedules, Notifications, Access Controls, Timeouts and Tags DatabricksTaskForPipelineTask Provides Databricks Task information to be used for the equivalent named task defined in your RTDIP Pipeline task <code>PipelineTask</code> DatabricksJobForPipelineJob Provides Databricks Job information to be used for the equivalent named Job defined in your RTDIP Pipeline Job <code>PipelineJob</code> <p>A simple example of deploying an RTDIP Pipeline Job to an Azure Databricks Job is below.</p> <pre><code>databricks_host_name = \"{databricks-host-url}\" #Replace with your databricks workspace url\n\n# Setup a Cluster for the Databricks Job\ndatabricks_job_cluster = DatabricksJobCluster(\n    job_cluster_key=\"test_job_cluster\", \n    new_cluster=DatabricksCluster(\n        spark_version = \"11.3.x-scala2.12\",\n        node_type_id = \"Standard_D3_v2\",\n        num_workers = 2\n    )\n)\n\n# Define a Databricks Task for the Pipeline Task\ndatabricks_task = DatabricksTaskForPipelineTask(name=\"test_task\", job_cluster_key=\"test_job_cluster\")\n\n# Create a Databricks Job for the Pipeline Job\ndatabricks_job = DatabricksJobForPipelineJob(\n    job_clusters=[databricks_job_cluster],\n    databricks_task_for_pipeline_task_list=[databricks_task]\n)\n\n# Deploy to Databricks\ndatabricks_job = DatabricksDBXDeploy(pipeline_job=pipeline_job, databricks_job_for_pipeline_job=databricks_job, host=databricks_host_name, token=access_token)\n\ndeploy_result = databricks_job.deploy()\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#launch","title":"Launch","text":"<p>Once a job is deployed to Databricks, it can be executed immediately using the following code.</p> <pre><code># Run/Launch the Job in Databricks\nlaunch_result = databricks_job.launch()\n</code></pre>"},{"location":"sdk/queries/connectors/","title":"Connectors","text":"<p>RTDIP SDK provides functionality to connect to and query its data using connectors. Below is a list of the available connectors.</p>"},{"location":"sdk/queries/connectors/#odbc","title":"ODBC","text":""},{"location":"sdk/queries/connectors/#databricks-sql-connector","title":"Databricks SQL Connector","text":"<p>Enables connectivity to Databricks using the Databricks SQL Connector which does not require any ODBC installation. </p> <p>For more information refer to this documentation and for the specific implementation within the RTDIP SDK, refer to this link</p>"},{"location":"sdk/queries/connectors/#example","title":"Example","text":"<pre><code>from rtdip_sdk.odbc import db_sql_connector\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = db_sql_connector.DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#pyodbc-sql-connector","title":"PYODBC SQL Connector","text":"<p>PYDOBC is a popular python package for querying data using ODBC. Refer to their documentation for more information about pyodbc and how you can leverage it in your code.</p> <p>View information about how pyodbc is implemented in the RTDIP SDK here.</p>"},{"location":"sdk/queries/connectors/#example_1","title":"Example","text":"<pre><code>from rtdip_sdk.odbc import pyodbc_sql_connector\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\ndriver_path = \"/Library/simba/spark/lib/libsparkodbc_sbu.dylib\"\n\nconnection = pyodbc_sql_connector.PYODBCSQLConnection(driver_path, sever_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#turbodbc-sql-connector","title":"TURBODBC SQL Connector","text":"<p>Turbodbc is a powerful python ODBC package that has advanced options for querying performance. Find out more about installing it on your operation system and what Turbodbc can do here and refer to this documentation for more information about how it is implemented in the RTDIP SDK.</p>"},{"location":"sdk/queries/connectors/#example_2","title":"Example","text":"<pre><code>from rtdip_sdk.odbc import turbodbc_sql_connector\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = turbodbc_sql_connector.TURBODBCSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/functions/","title":"Functions","text":"<p>The RTDIP SDK enables users to perform complex queries, including aggregation on datasets within the Platform. Please find below the various types of queries available for specific dataset types. These SDK Functions are also supported by the RTDIP API Docker Image.</p>"},{"location":"sdk/queries/functions/#time-series-events","title":"Time Series Events","text":""},{"location":"sdk/queries/functions/#raw","title":"Raw","text":"<p>Raw facilitates performing raw extracts of time series data, typically filtered by a Tag Name or Device Name and an event time.</p>"},{"location":"sdk/queries/functions/#resample","title":"Resample","text":"<p>Resample enables changing the frequency of time series observations. This is achieved by providing the following parameters:</p> <ul> <li>Sample Rate - The resampling rate</li> <li>Sample Unit - The resampling unit (second, minute, day, hour)</li> <li>Aggregation Method - Aggregrations including first, last, avg, min, max</li> </ul>"},{"location":"sdk/queries/functions/#interpolate","title":"Interpolate","text":"<p>Interpolate - takes resampling one step further to estimate the values of unknown data points that fall between existing, known data points. In addition to the resampling parameters, interpolation alse requires:</p> <ul> <li>Interpolation Method - Forward Fill or Backward Fill</li> </ul>"},{"location":"sdk/queries/functions/#time-weighted-averages","title":"Time Weighted Averages","text":"<p>Time Weighted Averages provide an unbiased average when working with irregularly sampled data. The RTDIP SDK requires the following parameters to perform time weighted average queries:</p> <ul> <li>Window Size Mins - Window size in minutes</li> <li>Window Length - Adds a longer window time for the start or end of specified date to cater for edge cases</li> <li>Step - Data points with step \"enabled\" or \"disabled\". The options for step are \"metadata\" (string), True or False (bool). For \"metadata\", the query requires that the TagName has a step column configured correctly in the meta data table</li> </ul>"},{"location":"sdk/queries/functions/#time-series-metadata","title":"Time Series Metadata","text":""},{"location":"sdk/queries/functions/#metadata","title":"Metadata","text":"<p>Metadata queries provide contextual information for time series measurements and include information such as names, descriptions and units of measure.</p> <p>Note</p> <p>RTDIP are continuously adding more to this list so check back regularly!</p>"},{"location":"sdk/queries/functions/#query-examples","title":"Query Examples","text":"<p>1. To use any of the RTDIP functions, use the commands below.</p> <pre><code>from rtdip_sdk.functions import resample\nfrom rtdip_sdk.functions import interpolate\nfrom rtdip_sdk.functions import raw\nfrom rtdip_sdk.functions import time_weighted_average\nfrom rtdip_sdk.functions import metadata\n</code></pre> <p>2. From functions you can use any of the following methods.</p>"},{"location":"sdk/queries/functions/#resample_1","title":"Resample","text":"<pre><code>resample.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/functions/#interpolate_1","title":"Interpolate","text":"<pre><code>interpolate.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/functions/#raw_1","title":"Raw","text":"<pre><code>raw.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/functions/#time-weighted-average","title":"Time Weighted Average","text":"<pre><code>time_weighted_average.get(connection, parameter_dict)\n</code></pre>"},{"location":"sdk/queries/functions/#metadata_1","title":"Metadata","text":"<pre><code>metadata.get(connection, parameter_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/","title":"Query Databricks SQL using the RTDIP SDK","text":"<p>This article provides a guide on how to use RTDIP SDK to query data via Databricks SQL. Before getting started, ensure you have installed the RTDIP Python Package and check the RTDIP Installation Page for all the required prerequisites.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#how-to-use-rtdip-sdk-with-databricks-sql","title":"How to use RTDIP SDK with Databricks SQL","text":"<p>The RTDIP SDK has rich support of querying data using Databricks SQL, such as allowing the user to authenticate, connect and/or use the most commonly requested methods for manipulating time series data accessible via Databricks SQL.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#authentication","title":"Authentication","text":"Azure Active DirectoryDatabricks <p>Refer to the Azure Active Directory documentation for further options to perform Azure AD authentication, such as Service Principal authentication using certificates or secrets. Below is an example of performing default authentication that retrieives a token for Azure Databricks. </p> <p>Also refer to the Code Reference for further technical information.</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>If you are experiencing any trouble authenticating please see Troubleshooting - Authentication</p> <p>Refer to the Databricks documentation for further information about generating a Databricks PAT Token. Below is an example of performing default authentication that retrieives a token for a Databricks Workspace. </p> <p>Provide your <code>dbapi.....</code> token to the <code>access_token</code> in the examples below.</p> <pre><code>access_token = \"dbapi..........\"\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#connect-to-databricks-sql","title":"Connect to Databricks SQL","text":"<p>The RTDIP SDK offers several ways to connect to a Databricks SQL Warehouse.</p> Databricks SQL ConnectorPYODBCTURBODBC <p>The simplest method to connect to RTDIP and does not require any additional installation steps.</p> <pre><code>from rtdip_sdk.odbc import db_sql_connector\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = db_sql_connector.DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p> <p>A popular library that python developers use for ODBC connectivity but requires more setup steps.</p> <p>ODBC or JDBC are required to leverage PYODBC. Follow these instructions to install the drivers in your environment.</p> <ul> <li> <p>Microsoft Visual C++ 14.0 or greater is required. Get it from Microsoft C++ Build Tools</p> </li> <li> <p>Driver paths can be found on PYODBC Driver Paths</p> </li> </ul> <pre><code>from rtdip_sdk.odbc import pyodbc_sql_connector\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\ndriver_path = \"/Library/simba/spark/lib/libsparkodbc_sbu.dylib\"\n\nconnection = pyodbc_sql_connector.PYODBCSQLConnection(driver_path, sever_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p> <p>The RTDIP development team have found this to be the most performant method of connecting to RTDIP leveraging the arrow implementation within Turbodbc to obtain data, but requires a number of addditional installation steps to get working on OSX, Linux and Windows</p> <ul> <li>ODBC or JDBC are required to leverage TURBODBC. Follow these instructions to install the drivers in your environment.</li> <li>Boost needs to be installed locally to use the TURBODBC SQL Connector (Optional)</li> </ul> <pre><code>from rtdip_sdk.odbc import turbodbc_sql_connector\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = turbodbc_sql_connector.TURBODBCSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#functions","title":"Functions","text":"<p>Finally, after authenticating and connecting using one of the methods above, you have access to the commonly requested RTDIP functions such as Resample, Interpolate, Raw, Time Weighted Averages or Metadata. </p> <p>1. To use any of the RTDIP functions, use the commands below.</p> <pre><code>from rtdip_sdk.functions import resample\nfrom rtdip_sdk.functions import interpolate\nfrom rtdip_sdk.functions import raw\nfrom rtdip_sdk.functions import time_weighted_average\nfrom rtdip_sdk.functions import metadata\n</code></pre> <p>2. From functions you can use any of the following methods.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#resample","title":"Resample","text":"<pre><code>resample.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#interpolate","title":"Interpolate","text":"<pre><code>interpolate.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#raw","title":"Raw","text":"<pre><code>raw.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#time-weighted-average","title":"Time Weighted Average","text":"<pre><code>time_weighted_average.get(connection, parameter_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#metadata","title":"Metadata","text":"<pre><code>metadata.get(connection, parameter_dict)\n</code></pre> <p>For more information about the function parameters see Code Reference and navigate through the required function.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#example","title":"Example","text":"<p>This is a code example of the RTDIP SDK Interpolate function. You will need to replace the parameters with your own requirements and details. If you are unsure on the options please see Code Reference - Interpolate and navigate to the attributes section. </p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\nfrom rtdip_sdk.odbc import db_sql_connector as dbc\nfrom rtdip_sdk.functions import interpolate\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = dbc.DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", access_token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\", \n    \"region\": \"{region}\",\n    \"asset\": \"{asset}\", \n    \"data_security_level\": \"{date_security_level}\",\n    \"data_type\": \"{data_type}\", #options are float, integer, string and double (the majority of data is float)\n    \"tag_names\": [\"{tag_name_1}, {tag_name_2}\"],\n    \"start_date\": \"2022-03-08\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2022-03-10\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"sample_rate\": \"1\", #numeric input\n    \"sample_unit\": \"hour\", #options are second, minute, day, hour\n    \"agg_method\": \"first\", #options are first, last, avg, min, max\n    \"interpolation_method\": \"forward_fill\", #options are forward_fill or backward_fill\n    \"include_bad_data\": True #boolean options are True or False\n}\n\nresult = interpolate.get(connection, parameters)\nprint(result)\n</code></pre> <p>Note</p> <p>If you are having problems please see Troubleshooting for more information.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#conclusion","title":"Conclusion","text":"<p>Congratulations! You have now learnt how to use the RTDIP SDK. Please check back for regular updates and if you would like to contribute, you can open an issue on GitHub. See the Contributing Guide for more help.</p>"},{"location":"sdk/queries/databricks/sql-warehouses/","title":"SQL Warehouses","text":"<p>In order to connect to the data using the RTDIP SDK you will require Databricks SQL Warehouse information. Retrieve this information from your Databricks Workspace by following the steps below:</p> <ol> <li>Login to your Databricks Workspace</li> <li>Switch to the SQL Option in the Workspace</li> <li>Select the SQL Warehouse</li> <li>Click on the Details tab</li> <li>Copy the Host Name and HTTP Path details</li> </ol>"},{"location":"sdk/queries/databricks/troubleshooting/","title":"Troubleshooting","text":""},{"location":"sdk/queries/databricks/troubleshooting/#cannot-install-pyodbc","title":"Cannot install pyodbc","text":"<p>Microsoft Visual C++ 14.0 or greater is required to install pyodbc. Get it with Microsoft C++ Build Tools</p>"},{"location":"sdk/queries/databricks/troubleshooting/#cannot-build-wheels-using-legacy-setuppy","title":"Cannot build wheels (Using legacy setup.py)","text":"<p>To install rtdip-sdk using setup.py, you need to have wheel installed using the following command:</p> <pre><code>pip install wheel\n</code></pre>"},{"location":"sdk/queries/databricks/troubleshooting/#authentication","title":"Authentication","text":"<p>For Default Credential authentication, a number of troublshooting options are available here.</p> <p>For Visual Studio Code errors, the version of Azure Account extension is installed(0.9.11) - To authenticate in Visual Studio Code, ensure version 0.9.11 or earlier of the Azure Account extension is installed. To track progress toward supporting newer extension versions, see this GitHub issue. Once installed, open the Command Palette and run the Azure: Sign In command</p>"},{"location":"sdk/queries/databricks/troubleshooting/#exception-has-occured-typeerror-module-object-is-not-callable","title":"Exception has occured: TypeError 'module' object is not callable","text":"<p>Ensure you are importing and using the RTDIP SDK functions correctly. You will need to give the module a name and reference it when using the function. See below for a code example. </p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\nfrom rtdip_sdk.odbc import db_sql_connector as dbc\nfrom rtdip_sdk.functions import interpolate\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = dbc.DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", access_token)\n\ndict = {\n    \"business_unit\": \"{business_unit}\", \n    \"region\": \"{region}\", \n    \"asset\": \"{asset}\", \n    \"data_security_level\": \"{date_security_level}\",\n    \"data_type\": \"{data_type}\", #options are float, integer, string and double (the majority of data is float)\n    \"tag_names\": [\"{tag_name_1}, {tag_name_2}\"],\n    \"start_date\": \"2022-03-08\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2022-03-10\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"sample_rate\": \"1\", #numeric input\n    \"sample_unit\": \"hour\", #options are second, minute, day, hour\n    \"agg_method\": \"first\", #options are first, last, avg, min, max\n    \"interpolation_method\": \"forward_fill\", #options are forward_fill or backward_fill\n    \"include_bad_data\": True #boolean options are True or False\n}\n\nresult = interpolate.get(connection, dict)\nprint(result)\n</code></pre>"},{"location":"sdk/queries/databricks/troubleshooting/#databricks-odbcjdbc-driver-issues","title":"Databricks ODBC/JDBC Driver issues","text":""},{"location":"sdk/queries/databricks/troubleshooting/#general-troubleshooting","title":"General Troubleshooting","text":"<p>Most issues related to the installation or performance of the ODBC/JDBC driver are documented here.</p>"},{"location":"sdk/queries/databricks/troubleshooting/#odbc-with-a-proxy","title":"ODBC with a proxy","text":"<p>Follow this document to use the ODBC driver with a proxy.  </p>"}]}